{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uR0mOtg3vCwfgrPdPZL4_wJQOYrp7A7E",
      "authorship_tag": "ABX9TyM7jFM9sObH4uGN0CEWFsdP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/Deep-learning-models/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def scale_dot_product(q, k, v, scale=None):\n",
        "  d_k = k.size()[-1]\n",
        "  scores = torch.matmul(q, k.transpose(-2, -1)) / np.sqrt(d_k)\n",
        "  if scale:\n",
        "    scores = scores.masked_fill(scale == 0, float('-inf'))\n",
        "  p_attn = F.softmax(scores, dim=-1)\n",
        "  values = torch.matmul(p_attn, v)\n",
        "  return values\n",
        "\n",
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, embed_dim, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.linear_Q = nn.Linear(embed_dim, dim_q)\n",
        "    self.linear_K = nn.Linear(embed_dim, dim_k)\n",
        "    self.linear_V = nn.Linear(embed_dim, dim_k)\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    return scale_dot_product(self.linear_Q(q), self.linear_K(k), self.linear_V(v), mask)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, dim_q, dim_k):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([AttentionHead(embed_dim, dim_q, dim_k) for _ in range(num_heads)])\n",
        "    self.linear = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, q, k, v, mask=None):\n",
        "    return self.linear(torch.cat([head(q, k, v, mask) for head in self.heads], dim=-1))\n",
        "\n",
        "\n",
        "def feed_forward(embed_dim, hidden_dim) -> nn.Sequential:\n",
        "  return nn.Sequential(\n",
        "      nn.Linear(embed_dim, hidden_dim),\n",
        "      nn.ReLU(),\n",
        "      nn.Linear(hidden_dim, embed_dim)\n",
        "  )\n",
        "\n",
        "\n",
        "class Residual(nn.Module):\n",
        "  def __init__(self, sublayer:nn.Module, dimension, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.sublayer = sublayer\n",
        "    self.norm = nn.LayerNorm(dimension)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x, *args, **kwargs):\n",
        "    return self.norm(x + self.dropout(self.sublayer(x, *args, **kwargs)))\n",
        "\n",
        "def positional_encoding(max_len, d_model):\n",
        "  def get_angle(pos, i, d_model):\n",
        "    angle_rate = 1 / np.power(10000, 2 * (i // 2) / d_model)\n",
        "    return pos * angle_rate\n",
        "\n",
        "  angle_rads = get_angle(np.arange(max_len)[:, np.newaxis],\n",
        "               np.arange(d_model)[np.newaxis, :],\n",
        "               d_model)\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "  return torch.tensor(pos_encoding, dtype=torch.float32)\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    query_dim = key_dim = max(embed_dim // num_heads, 1)\n",
        "    self.multi_head_attention = Residual(\n",
        "        MultiHeadAttention(embed_dim, num_heads, query_dim, key_dim),\n",
        "        embed_dim,\n",
        "        dropout)\n",
        "    self.feed_forward = Residual(\n",
        "        feed_forward(embed_dim, ffn_dim),\n",
        "        embed_dim,\n",
        "        dropout)\n",
        "\n",
        "  def forward(self, src, mask):\n",
        "    return self.feed_forward(self.multi_head_attention(src, src, src, mask))\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, num_layers, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([TransformerEncoderLayer(embed_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
        "\n",
        "  def forward(self, x, mask=None):\n",
        "    seq_len = x.size(1)\n",
        "    dimension=x.size(2)\n",
        "    x = x + positional_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return x\n",
        "\n",
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    head_dim = embed_dim // num_heads\n",
        "    self.self_attention = Residual(\n",
        "        MultiHeadAttention(embed_dim, num_heads, head_dim, head_dim),\n",
        "        embed_dim,\n",
        "        dropout)\n",
        "    self.cross_attention = Residual(\n",
        "        MultiHeadAttention(embed_dim, num_heads, head_dim, head_dim),\n",
        "        embed_dim,\n",
        "        dropout)\n",
        "    self.feed_forward = Residual(\n",
        "        feed_forward(embed_dim, ffn_dim),\n",
        "        embed_dim,\n",
        "        dropout)\n",
        "\n",
        "  def forward(self, src, memory):\n",
        "    target = self.self_attention(src, src, src)\n",
        "    target = self.cross_attention(target, memory, memory)\n",
        "    target = self.feed_forward(target)\n",
        "    return target\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, num_layers, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.layers = nn.ModuleList([TransformerDecoderLayer(embed_dim, num_heads, ffn_dim, dropout) for _ in range(num_layers)])\n",
        "    self.final_layer = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "  def forward(self, x, memory):\n",
        "    seq_len = x.size(1)\n",
        "    dimension=x.size(2)\n",
        "    x = x + positional_encoding(seq_len, dimension)\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, memory)\n",
        "    return torch.softmax(self.final_layer(x), dim=-1)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  def __init__(self, num_layers, embed_dim, num_heads, ffn_dim, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ffn_dim, dropout)\n",
        "    self.decoder = TransformerDecoder(num_layers, embed_dim, num_heads, ffn_dim, dropout)\n",
        "\n",
        "  def forward(self, src, target):\n",
        "    return self.decoder(target, self.encoder(src))\n",
        "\n",
        "batch_size=2\n",
        "seq_len=4\n",
        "embd_dim=8\n",
        "\n",
        "src = torch.rand(batch_size, seq_len, embd_dim)\n",
        "tgt = torch.rand(batch_size, seq_len, embd_dim)\n",
        "\n",
        "transformer = Transformer(2, embd_dim, 4, 512)\n",
        "out = transformer(src, tgt)\n",
        "\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-K-BdBeiLGa",
        "outputId": "1869b7be-8adb-4dd6-f651-9d83bc04c9a6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.2240, 0.2658, 0.0978, 0.0648, 0.0655, 0.1634, 0.0594, 0.0593],\n",
            "         [0.2318, 0.2820, 0.0737, 0.0478, 0.0701, 0.1623, 0.0614, 0.0711],\n",
            "         [0.3643, 0.1717, 0.0644, 0.0433, 0.0843, 0.0924, 0.0957, 0.0838],\n",
            "         [0.2160, 0.2720, 0.0575, 0.0559, 0.1137, 0.0989, 0.0942, 0.0918]],\n",
            "\n",
            "        [[0.2272, 0.2034, 0.0989, 0.0723, 0.1048, 0.1242, 0.0772, 0.0920],\n",
            "         [0.2709, 0.1994, 0.1149, 0.0374, 0.0469, 0.1996, 0.0635, 0.0672],\n",
            "         [0.2017, 0.2876, 0.0751, 0.0522, 0.0793, 0.1472, 0.0800, 0.0769],\n",
            "         [0.1946, 0.1486, 0.0787, 0.0571, 0.1171, 0.0762, 0.2135, 0.1143]]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    }
  ]
}