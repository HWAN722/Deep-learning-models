{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 88046,
          "databundleVersionId": 10229277,
          "sourceType": "competition"
        },
        {
          "sourceId": 104492,
          "sourceType": "modelInstanceVersion",
          "isSourceIdPinned": true,
          "modelInstanceId": 72255,
          "modelId": 76277
        },
        {
          "sourceId": 120005,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 100936,
          "modelId": 121027
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Santa",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/Deep-learning-models/blob/main/Kaggle/Santa%202024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "aqHN7yiPo-pA"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "santa_2024_path = kagglehub.competition_download('santa-2024')\n",
        "google_gemma_2_transformers_gemma_2_9b_2_path = kagglehub.model_download('google/gemma-2/Transformers/gemma-2-9b/2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "RrNWb6EBo-pD"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Santa 2024 - The Perplexity Permutation Puzzle\n",
        "\n",
        "Minimizing the perplexity of given string"
      ],
      "metadata": {
        "id": "7hTf3YpSo-pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import pandas as pd\n",
        "import re\n",
        "import math\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from heapq import heappush, heappop\n",
        "\n",
        "random.seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-22T18:48:41.373707Z",
          "iopub.execute_input": "2025-01-22T18:48:41.374Z",
          "iopub.status.idle": "2025-01-22T18:48:49.668368Z",
          "shell.execute_reply.started": "2025-01-22T18:48:41.373976Z",
          "shell.execute_reply": "2025-01-22T18:48:49.667424Z"
        },
        "id": "IAeNZAfoo-pG",
        "outputId": "bfab603e-22f4-43d2-89c1-1a8ac756845e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "cuda\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# damn Llama cannot do the rearrange job\n",
        "# model_name = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:30:12.379472Z",
          "iopub.execute_input": "2025-01-20T22:30:12.379971Z",
          "iopub.status.idle": "2025-01-20T22:30:12.383374Z",
          "shell.execute_reply.started": "2025-01-20T22:30:12.379935Z",
          "shell.execute_reply": "2025-01-20T22:30:12.382519Z"
        },
        "id": "mNo5xQ76o-pH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Evaluation metric for Santa 2024.\"\"\"\n",
        "# https://www.kaggle.com/code/metric/santa-2024-metric/notebook\n",
        "import gc\n",
        "import os\n",
        "from math import exp\n",
        "from collections import Counter\n",
        "from typing import List, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "os.environ['OMP_NUM_THREADS'] = '1'\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
        "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class ParticipantVisibleError(Exception):\n",
        "    pass\n",
        "\n",
        "\n",
        "def score(\n",
        "    solution: pd.DataFrame,\n",
        "    submission: pd.DataFrame,\n",
        "    row_id_column_name: str,\n",
        "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
        "    load_in_8bit: bool = False,\n",
        "    clear_mem: bool = False,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    solution : DataFrame\n",
        "        DataFrame containing the original text in a column named 'text'.\n",
        "        Includes a row ID column specified by `row_id_column_name`.\n",
        "\n",
        "    submission : DataFrame\n",
        "        DataFrame containing the permuted text in a column named 'text'.\n",
        "        Must have the same row IDs as the solution.\n",
        "        Includes a row ID column specified by `row_id_column_name`.\n",
        "\n",
        "    row_id_column_name : str\n",
        "        Name of the column containing row IDs.\n",
        "        Ensures aligned comparison between solution and submission.\n",
        "\n",
        "    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n",
        "        Path to the serialized LLM.\n",
        "\n",
        "    load_in_8bit : bool, default=False\n",
        "        Use 8-bit quantization for the model. Requires CUDA.\n",
        "\n",
        "    clear_mem : bool, default=False\n",
        "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
        "        Useful for testing.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The mean perplexity score. Lower is better.\n",
        "\n",
        "    Raises\n",
        "    ------\n",
        "    ParticipantVisibleError\n",
        "        If the submission format is invalid or submitted strings are not valid permutations.\n",
        "\n",
        "    Examples\n",
        "    --------\n",
        "    >>> import pandas as pd\n",
        "    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "    >>> solution = pd.DataFrame({\n",
        "    ...     'id': [0, 1],\n",
        "    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
        "    ... })\n",
        "    >>> submission = pd.DataFrame({\n",
        "    ...     'id': [0, 1],\n",
        "    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
        "    ... })\n",
        "    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
        "    True\n",
        "    \"\"\"\n",
        "    # Check that each submitted string is a permutation of the solution string\n",
        "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
        "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
        "    invalid_mask = sol_counts != sub_counts\n",
        "    if invalid_mask.any():\n",
        "        raise ParticipantVisibleError(\n",
        "            'At least one submitted string is not a valid permutation of the solution string.'\n",
        "        )\n",
        "\n",
        "    # Calculate perplexity for the submitted strings\n",
        "    sub_strings = [\n",
        "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
        "    ]  # Split and rejoin to normalize whitespace\n",
        "    scorer = PerplexityCalculator(\n",
        "        model_path=model_path,\n",
        "        load_in_8bit=load_in_8bit,\n",
        "    )  # Initialize the perplexity calculator with a pre-trained model\n",
        "    perplexities = scorer.get_perplexity(\n",
        "        sub_strings\n",
        "    )  # Calculate perplexity for each submitted string\n",
        "\n",
        "    if clear_mem:\n",
        "        # Just move on if it fails. Not essential if we have the score.\n",
        "        try:\n",
        "            scorer.clear_gpu_memory()\n",
        "        except:\n",
        "            print('GPU memory clearing failed.')\n",
        "\n",
        "    return float(np.mean(perplexities))\n",
        "\n",
        "\n",
        "class PerplexityCalculator:\n",
        "    \"\"\"\n",
        "    Calculates perplexity of text using a pre-trained language model.\n",
        "\n",
        "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_path : str\n",
        "        Path to the pre-trained language model\n",
        "\n",
        "    load_in_8bit : bool, default=False\n",
        "        Use 8-bit quantization for the model. Requires CUDA.\n",
        "\n",
        "    device_map : str, default=\"auto\"\n",
        "        Device mapping for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_path: str,\n",
        "        load_in_8bit: bool = False,\n",
        "        device_map: str = 'auto',\n",
        "    ):\n",
        "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
        "        # Configure model loading based on quantization setting and device availability\n",
        "        if load_in_8bit:\n",
        "            if DEVICE.type != 'cuda':\n",
        "                raise ValueError('8-bit quantization requires CUDA device')\n",
        "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=device_map,\n",
        "            )\n",
        "        else:\n",
        "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "                model_path,\n",
        "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
        "                device_map=device_map,\n",
        "            )\n",
        "\n",
        "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "    def get_perplexity(\n",
        "        self, input_texts: Union[str, List[str]], debug=False\n",
        "    ) -> Union[float, List[float]]:\n",
        "        \"\"\"\n",
        "        Calculates the perplexity of given texts.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        input_texts : str or list of str\n",
        "            A single string or a list of strings.\n",
        "\n",
        "        batch_size : int, default=None\n",
        "            Batch size for processing. Defaults to the number of input texts.\n",
        "\n",
        "        debug : bool, default=False\n",
        "            Print debugging information.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        float or list of float\n",
        "            A single perplexity value if input is a single string,\n",
        "            or a list of perplexity values if input is a list of strings.\n",
        "\n",
        "        Examples\n",
        "        --------\n",
        "        >>> import pandas as pd\n",
        "        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
        "        >>> scorer = PerplexityCalculator(model_path=model_path)\n",
        "\n",
        "        >>> submission = pd.DataFrame({\n",
        "        ...     'id': [0, 1, 2],\n",
        "        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
        "        ... })\n",
        "        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
        "        >>> perplexities[0] < perplexities[1]\n",
        "        True\n",
        "        >>> perplexities[2] < perplexities[0]\n",
        "        True\n",
        "\n",
        "        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
        "        >>> all(p > 0 for p in perplexities)\n",
        "        True\n",
        "\n",
        "        >>> scorer.clear_gpu_memory()\n",
        "        \"\"\"\n",
        "        single_input = isinstance(input_texts, str)\n",
        "        input_texts = [input_texts] if single_input else input_texts\n",
        "\n",
        "        loss_list = []\n",
        "        with torch.no_grad():\n",
        "            # Process each sequence independently\n",
        "            for text in input_texts:\n",
        "                # Explicitly add sequence boundary tokens to the text\n",
        "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Tokenize\n",
        "                model_inputs = self.tokenizer(\n",
        "                    text_with_special,\n",
        "                    return_tensors='pt',\n",
        "                    add_special_tokens=False,\n",
        "                )\n",
        "\n",
        "                if 'token_type_ids' in model_inputs:\n",
        "                    model_inputs.pop('token_type_ids')\n",
        "\n",
        "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
        "\n",
        "                # Get model output\n",
        "                output = self.model(**model_inputs, use_cache=False)\n",
        "                logits = output['logits']\n",
        "\n",
        "                # Shift logits and labels for calculating loss\n",
        "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
        "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
        "\n",
        "                # Calculate token-wise loss\n",
        "                loss = self.loss_fct(\n",
        "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                    shift_labels.view(-1)\n",
        "                )\n",
        "\n",
        "                # Calculate average loss\n",
        "                sequence_loss = loss.sum() / len(loss)\n",
        "                loss_list.append(sequence_loss.cpu().item())\n",
        "\n",
        "                # Debug output\n",
        "                if debug:\n",
        "                    print(f\"\\nProcessing: '{text}'\")\n",
        "                    print(f\"With special tokens: '{text_with_special}'\")\n",
        "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
        "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
        "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
        "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
        "                    print(f\"Individual losses: {loss.tolist()}\")\n",
        "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
        "\n",
        "        ppl = [exp(i) for i in loss_list]\n",
        "\n",
        "        if debug:\n",
        "            print(\"\\nFinal perplexities:\")\n",
        "            for text, perp in zip(input_texts, ppl):\n",
        "                print(f\"Text: '{text}'\")\n",
        "                print(f\"Perplexity: {perp:.2f}\")\n",
        "\n",
        "        return ppl[0] if single_input else ppl\n",
        "\n",
        "    def clear_gpu_memory(self) -> None:\n",
        "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return\n",
        "\n",
        "        # Delete model and tokenizer if they exist\n",
        "        if hasattr(self, 'model'):\n",
        "            del self.model\n",
        "        if hasattr(self, 'tokenizer'):\n",
        "            del self.tokenizer\n",
        "\n",
        "        # Run garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache and reset memory stats\n",
        "        with DEVICE:\n",
        "            torch.cuda.empty_cache()\n",
        "            torch.cuda.ipc_collect()\n",
        "            torch.cuda.reset_peak_memory_stats()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-22T18:48:56.611861Z",
          "iopub.execute_input": "2025-01-22T18:48:56.612213Z",
          "iopub.status.idle": "2025-01-22T18:48:56.629381Z",
          "shell.execute_reply.started": "2025-01-22T18:48:56.612183Z",
          "shell.execute_reply": "2025-01-22T18:48:56.628546Z"
        },
        "id": "-Y1My-5jo-pI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "scorer = PerplexityCalculator(\"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-22T18:49:04.024656Z",
          "iopub.execute_input": "2025-01-22T18:49:04.024961Z",
          "iopub.status.idle": "2025-01-22T18:52:11.598882Z",
          "shell.execute_reply.started": "2025-01-22T18:49:04.024938Z",
          "shell.execute_reply": "2025-01-22T18:52:11.598007Z"
        },
        "id": "bOttgaJVo-pJ",
        "outputId": "d81a8dc6-655c-4c2e-c8ee-c8b0c2a10108",
        "colab": {
          "referenced_widgets": [
            "5463352ecd184bbab446a2f48e089be8"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5463352ecd184bbab446a2f48e089be8"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# annealing\n",
        "import random\n",
        "import math\n",
        "\n",
        "\n",
        "def get_neighbor(sequence, vocabulary):\n",
        "    new_sequence = sequence[:]\n",
        "    idx1, idx2 = random.sample(range(len(sequence)), 2)\n",
        "    new_sequence[idx1], new_sequence[idx2] = new_sequence[idx2], new_sequence[idx1]\n",
        "    return new_sequence\n",
        "\n",
        "def simulated_annealing(vocabulary, sequence_length, initial_temperature, cooling_rate, num_iterations):\n",
        "    random.shuffle(vocabulary)\n",
        "    current_sequence = vocabulary\n",
        "    current_perplexity = scorer.get_perplexity(\" \".join(current_sequence))\n",
        "    best_sequence = current_sequence\n",
        "    best_perplexity = current_perplexity\n",
        "\n",
        "    temperature = initial_temperature\n",
        "\n",
        "    for iteration in range(num_iterations):\n",
        "        neighbor_sequence = get_neighbor(current_sequence, vocabulary)\n",
        "        neighbor_perplexity = scorer.get_perplexity(\" \".join(neighbor_sequence))\n",
        "\n",
        "        if (neighbor_perplexity < current_perplexity or\n",
        "            math.exp(-(neighbor_perplexity - current_perplexity) / temperature) > random.random()):\n",
        "            current_sequence = neighbor_sequence\n",
        "            current_perplexity = neighbor_perplexity\n",
        "\n",
        "        if neighbor_perplexity < best_perplexity:\n",
        "            best_sequence = neighbor_sequence\n",
        "            best_perplexity = neighbor_perplexity\n",
        "\n",
        "        temperature *= cooling_rate\n",
        "        if iteration%500 == 0:\n",
        "            print(math.exp(-(neighbor_perplexity - current_perplexity) / temperature))\n",
        "            print(f\"Iteration {iteration}: Best Perplexity {best_perplexity}, Current Perplexity {current_perplexity}, Temperature {temperature}\")\n",
        "\n",
        "    return best_sequence, best_perplexity\n",
        "\n",
        "string = 'from and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night paper peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping'\n",
        "vocabulary = string.split(\" \")\n",
        "sequence_length = 50\n",
        "initial_temperature = 100.0\n",
        "cooling_rate = 0.9\n",
        "num_iterations = 3000\n",
        "\n",
        "best_sequence, best_perplexity = simulated_annealing(vocabulary, sequence_length, initial_temperature, cooling_rate, num_iterations)\n",
        "print(f\"Best sequence: {' '.join(best_sequence)} with perplexity {best_perplexity}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-22T20:00:27.982734Z",
          "iopub.execute_input": "2025-01-22T20:00:27.983223Z",
          "iopub.status.idle": "2025-01-22T20:06:51.798986Z",
          "shell.execute_reply.started": "2025-01-22T20:00:27.983183Z",
          "shell.execute_reply": "2025-01-22T20:06:51.798011Z"
        },
        "id": "8fzr9Mj6o-pK",
        "outputId": "e28830db-1f3e-40e6-f4e5-ab72e7b3bd03"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "1.0\nIteration 0: Best Perplexity 1787.3692378744172, Current Perplexity 1803.7213585750446, Temperature 90.0\n0.0\nIteration 500: Best Perplexity 592.4852517510855, Current Perplexity 592.4852517510855, Temperature 1.18986373753274e-21\n0.0\nIteration 1000: Best Perplexity 538.4819550392915, Current Perplexity 538.4819550392915, Temperature 1.5730841265504226e-44\n0.0\nIteration 1500: Best Perplexity 443.6802820305299, Current Perplexity 443.6802820305299, Temperature 2.0797286203007897e-67\n0.0\nIteration 2000: Best Perplexity 389.9301635161782, Current Perplexity 389.9301635161782, Temperature 2.7495485213387876e-90\n0.0\nIteration 2500: Best Perplexity 374.300102229063, Current Perplexity 374.300102229063, Temperature 3.635097866808655e-113\nBest sequence: wreath to have hope candle workshop cookie and from candy fruitcake eggnog bow toy doll game night greeting card puzzle in that it not chocolate milk peppermint with as you dream believe wonder wish merry we the fireplace wrapping kaggle paper star angel of peace snowglobe poinsettia joy season hohoho with perplexity 367.1973803833881\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# beam search\n",
        "def heuristic_rearrange(words, scorer, beam_size=5):\n",
        "    sequences = [([], words)]\n",
        "    cache = {}\n",
        "\n",
        "    for _ in range(len(words)):\n",
        "        all_candidates = []\n",
        "        for seq, remaining in sequences:\n",
        "            candidates = []\n",
        "            candidate_texts = []\n",
        "            candidate_data = []\n",
        "\n",
        "            for i, word in enumerate(remaining):\n",
        "                new_seq = seq + [word]\n",
        "                new_remaining = remaining[:i] + remaining[i+1:]\n",
        "                seq_key = ' '.join(new_seq)\n",
        "\n",
        "                if seq_key in cache:\n",
        "                    perplexity = cache[seq_key]\n",
        "                    all_candidates.append((perplexity, new_seq, new_remaining))\n",
        "                else:\n",
        "                    candidate_texts.append(seq_key)\n",
        "                    candidate_data.append((new_seq, new_remaining, seq_key))\n",
        "\n",
        "            if candidate_texts:\n",
        "                perplexities = scorer.get_perplexity(candidate_texts)\n",
        "                perplexities = list(map(int, perplexities))\n",
        "                for (candidate_seq, candidate_remaining, seq_key), perplexity in zip(candidate_data, perplexities):\n",
        "                    cache[seq_key] = perplexity\n",
        "                    all_candidates.append((perplexity, candidate_seq, candidate_remaining))\n",
        "\n",
        "        ordered = sorted(all_candidates, key=lambda x: x[0])\n",
        "        # print(ordered)\n",
        "        # print(beam_size)\n",
        "        sequences = [(seq, rem) for _, seq, rem in ordered[:beam_size]]\n",
        "\n",
        "    best_seq = sequences[0][0]\n",
        "    return ' '.join(best_seq)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-22T02:29:10.05832Z",
          "iopub.execute_input": "2025-01-22T02:29:10.058626Z",
          "iopub.status.idle": "2025-01-22T02:29:10.065041Z",
          "shell.execute_reply.started": "2025-01-22T02:29:10.0586Z",
          "shell.execute_reply": "2025-01-22T02:29:10.06425Z"
        },
        "id": "1PVEAfdno-pL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the heuristic_rearrange 5 solution\n",
        "math = [\n",
        "    'reindeer mistletoe scrooge gingerbread elf fireplace chimney family advent ornament',\n",
        "    'reindeer mistletoe scrooge and the gingerbread family fireplace elf night walk advent ornament chimney bake sleep laugh jump drive give',\n",
        "    'jingle yuletide carol grinch nutcracker holiday decorations ornament stocking gifts naughty nice holly cheer sleigh beard chimney workshop magi polar',\n",
        "    'yuletide cheer cheer and sing of the carol grinch nutcracker holiday decorations gifts stocking unwrap ornament jingle sleigh holly nice naughty chimney visit beard workshop polar relax eat is magi',\n",
        "    'eggnog cookie poinsettia fruitcake chocolate peppermint candy snowglobe wreath and star candle angel card paper doll game night in with the hohoho season of joy greeting from you to we hope that wish it merry have peace wonder believe dream not as wrapping bow toy workshop fireplace milk kaggle puzzle',\n",
        "    'eggnog yuletide scrooge mistletoe nutcracker poinsettia gingerbread cookie fruitcake holly wreath holiday ornament snowglobe candle fireplace stocking fireplace chimney reindeer elf toy sleigh gifts candy peppermint chocolate ornament decorations advent season family merry and grinch joy peace cheer cheer and hope greeting card and wrapping paper wish you the wonder of the night star night in the dream of it is nice to believe that we have not sleep with angel visit carol sing jingle unwrap give eat bake laugh walk drive jump hohoho from chimney doll workshop workshop puzzle game naughty milk beard polar bow relax as magi kaggle']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.245094Z",
          "iopub.status.idle": "2025-01-20T22:32:55.245373Z",
          "shell.execute_reply": "2025-01-20T22:32:55.245259Z"
        },
        "id": "SePcLIpPo-pL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "current_best = [\n",
        "    'reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament',\n",
        "    'reindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament',\n",
        "    'sleigh yuletide beard carol cheer chimney decorations gifts grinch holiday holly jingle magi naughty nice nutcracker ornament polar workshop stocking',\n",
        "    'sleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax sing carol visit workshop grinch naughty nice chimney stocking ornament nutcracker polar beard',\n",
        "    'from and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night paper peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping',\n",
        "    'from and and as we and have the in is it of not that the to with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game gifts give gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide']"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.247306Z",
          "iopub.status.idle": "2025-01-20T22:32:55.247602Z",
          "shell.execute_reply": "2025-01-20T22:32:55.24748Z"
        },
        "id": "HqiiJ5dQo-pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# random rearrange\n",
        "def rearrange_words(words,best_score, max_iterations=20000):\n",
        "    \"\"\"What I can do, let's go random\"\"\"\n",
        "    min_perplexity = float('inf')\n",
        "    best_sentence = ''\n",
        "    tried_permutations = set()\n",
        "    words = words.split(\" \")\n",
        "    # print(len(words))\n",
        "    for _ in range(max_iterations):\n",
        "        # Generate a random permutation\n",
        "        permuted_words = tuple(torch.randperm(len(words)).tolist())\n",
        "        if permuted_words in tried_permutations:\n",
        "            continue\n",
        "        tried_permutations.add(permuted_words)\n",
        "\n",
        "        sentence = ' '.join([words[i] for i in permuted_words])\n",
        "        perplexity = scorer.get_perplexity(sentence)\n",
        "        if perplexity < best_score:\n",
        "            print(sentence)\n",
        "            print(perplexity)\n",
        "        if perplexity < min_perplexity:\n",
        "            min_perplexity = perplexity\n",
        "            best_sentence = sentence\n",
        "\n",
        "    return best_sentence, min_perplexity"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.248502Z",
          "iopub.status.idle": "2025-01-20T22:32:55.248827Z",
          "shell.execute_reply": "2025-01-20T22:32:55.248677Z"
        },
        "id": "1NTF8Kxxo-pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import random\n",
        "def swap_until_perplexity_decreases(words, current_perplexity, scorer):\n",
        "    print(f\"Initial perplexity: {current_perplexity}\")\n",
        "    i = 0\n",
        "    tried_swaps = set()\n",
        "    all_possible_swaps = list(itertools.combinations(range(len(words)), 2))\n",
        "    random.shuffle(all_possible_swaps)\n",
        "\n",
        "    for index1, index2 in all_possible_swaps:\n",
        "        words[index1], words[index2] = words[index2], words[index1]\n",
        "\n",
        "        new_perplexity = scorer.get_perplexity(\" \".join(words))\n",
        "\n",
        "        if new_perplexity < current_perplexity:\n",
        "            print(\" \".join(words))\n",
        "            print(new_perplexity)\n",
        "            return \" \".join(words)\n",
        "        else:\n",
        "            words[index1], words[index2] = words[index2], words[index1]\n",
        "    print(\"No swap could reduce perplexity.\")\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.249658Z",
          "iopub.status.idle": "2025-01-20T22:32:55.249992Z",
          "shell.execute_reply": "2025-01-20T22:32:55.24985Z"
        },
        "id": "L88BQf91o-pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')\n",
        "\n",
        "results = {'id': [], 'text': []}\n",
        "\n",
        "for idx, row in tqdm(submission.iterrows(), total=submission.shape[0]):\n",
        "    text_id = row['id']\n",
        "    scrambled_text = row['text']\n",
        "\n",
        "    # rearranged_text_math = heuristic_rearrange(scrambled_text.split(\" \"), scorer, 1)\n",
        "    rearranged_text_best = current_best[idx]\n",
        "    best_score = scorer.get_perplexity(rearranged_text_best)\n",
        "    swap_until_perplexity_decreases(scrambled_text.split(\" \"), best_score, scorer)\n",
        "\n",
        "    # print(rearranged_text_math)\n",
        "    # best_score = scorer.get_perplexity(rearranged_text_best)\n",
        "    # rearranged_text_random, random_score = rearrange_words(scrambled_text, best_score)\n",
        "    # print(rearranged_text_random)\n",
        "    min_score = float('inf')\n",
        "    # math_score = scorer.get_perplexity(rearranged_text_math)\n",
        "    # best_score = scorer.get_perplexity(rearranged_text_best)\n",
        "    # if math_score < min_score:\n",
        "    #     min_score = math_score\n",
        "    #     rearranged_text = rearranged_text_math\n",
        "    # if random_score < min_score:\n",
        "    #     min_score = random_score\n",
        "    #     rearranged_text = rearranged_text_random\n",
        "    if best_score < min_score:\n",
        "        min_score = best_score\n",
        "        rearranged_text = rearranged_text_best # I made a mistake here :(\n",
        "    # print(math_score)\n",
        "    # print(best_score)\n",
        "    # print(random_score)\n",
        "    results['id'].append(text_id)\n",
        "    results['text'].append(rearranged_text)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.250653Z",
          "iopub.status.idle": "2025-01-20T22:32:55.250912Z",
          "shell.execute_reply": "2025-01-20T22:32:55.250809Z"
        },
        "id": "gSzaMQiqo-pM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# save to submission.csv\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.head()\n",
        "output_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-01-20T22:32:55.251865Z",
          "iopub.status.idle": "2025-01-20T22:32:55.252218Z",
          "shell.execute_reply": "2025-01-20T22:32:55.252051Z"
        },
        "id": "WINsvmzEo-pM"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}