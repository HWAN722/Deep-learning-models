{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd5ae31",
   "metadata": {
    "papermill": {
     "duration": 0.003424,
     "end_time": "2025-01-27T19:49:35.848666",
     "exception": false,
     "start_time": "2025-01-27T19:49:35.845242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Santa 2024 - The Perplexity Permutation Puzzle\n",
    "\n",
    "Minimizing the perplexity of given string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03c66d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T19:49:35.855576Z",
     "iopub.status.busy": "2025-01-27T19:49:35.855252Z",
     "iopub.status.idle": "2025-01-27T19:49:44.554773Z",
     "shell.execute_reply": "2025-01-27T19:49:44.553723Z"
    },
    "papermill": {
     "duration": 8.704811,
     "end_time": "2025-01-27T19:49:44.556347",
     "exception": false,
     "start_time": "2025-01-27T19:49:35.851536",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from heapq import heappush, heappop\n",
    "\n",
    "random.seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f8b7b2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T19:49:44.563247Z",
     "iopub.status.busy": "2025-01-27T19:49:44.562804Z",
     "iopub.status.idle": "2025-01-27T19:49:44.566045Z",
     "shell.execute_reply": "2025-01-27T19:49:44.565416Z"
    },
    "papermill": {
     "duration": 0.007891,
     "end_time": "2025-01-27T19:49:44.567264",
     "exception": false,
     "start_time": "2025-01-27T19:49:44.559373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# damn Llama cannot do the rearrange job\n",
    "# model_name = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94291c52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T19:49:44.573618Z",
     "iopub.status.busy": "2025-01-27T19:49:44.573370Z",
     "iopub.status.idle": "2025-01-27T19:49:44.591147Z",
     "shell.execute_reply": "2025-01-27T19:49:44.590420Z"
    },
    "papermill": {
     "duration": 0.022511,
     "end_time": "2025-01-27T19:49:44.592465",
     "exception": false,
     "start_time": "2025-01-27T19:49:44.569954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"Evaluation metric for Santa 2024.\"\"\"\n",
    "# https://www.kaggle.com/code/metric/santa-2024-metric/notebook\n",
    "import gc\n",
    "import os\n",
    "from math import exp\n",
    "from collections import Counter\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "PAD_TOKEN_LABEL_ID = torch.nn.CrossEntropyLoss().ignore_index\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "class ParticipantVisibleError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "def score(\n",
    "    solution: pd.DataFrame,\n",
    "    submission: pd.DataFrame,\n",
    "    row_id_column_name: str,\n",
    "    model_path: str = '/kaggle/input/gemma-2/transformers/gemma-2-9b/2',\n",
    "    load_in_8bit: bool = False,\n",
    "    clear_mem: bool = False,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the mean perplexity of submitted text permutations compared to an original text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    solution : DataFrame\n",
    "        DataFrame containing the original text in a column named 'text'.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    submission : DataFrame\n",
    "        DataFrame containing the permuted text in a column named 'text'.\n",
    "        Must have the same row IDs as the solution.\n",
    "        Includes a row ID column specified by `row_id_column_name`.\n",
    "\n",
    "    row_id_column_name : str\n",
    "        Name of the column containing row IDs.\n",
    "        Ensures aligned comparison between solution and submission.\n",
    "\n",
    "    model_path : str, default='/kaggle/input/gemma-2/transformers/gemma-2-9b/2'\n",
    "        Path to the serialized LLM.\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    clear_mem : bool, default=False\n",
    "        Clear GPU memory after scoring by clearing the CUDA cache.\n",
    "        Useful for testing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The mean perplexity score. Lower is better.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ParticipantVisibleError\n",
    "        If the submission format is invalid or submitted strings are not valid permutations.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import pandas as pd\n",
    "    >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "    >>> solution = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"this is a normal english sentence\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "    ... })\n",
    "    >>> submission = pd.DataFrame({\n",
    "    ...     'id': [0, 1],\n",
    "    ...     'text': [\"sentence english normal a is this\", \"lazy the over jumps fox brown quick the dog\"]\n",
    "    ... })\n",
    "    >>> score(solution, submission, 'id', model_path=model_path, clear_mem=True) > 0\n",
    "    True\n",
    "    \"\"\"\n",
    "    # Check that each submitted string is a permutation of the solution string\n",
    "    sol_counts = solution.loc[:, 'text'].str.split().apply(Counter)\n",
    "    sub_counts = submission.loc[:, 'text'].str.split().apply(Counter)\n",
    "    invalid_mask = sol_counts != sub_counts\n",
    "    if invalid_mask.any():\n",
    "        raise ParticipantVisibleError(\n",
    "            'At least one submitted string is not a valid permutation of the solution string.'\n",
    "        )\n",
    "\n",
    "    # Calculate perplexity for the submitted strings\n",
    "    sub_strings = [\n",
    "        ' '.join(s.split()) for s in submission['text'].tolist()\n",
    "    ]  # Split and rejoin to normalize whitespace\n",
    "    scorer = PerplexityCalculator(\n",
    "        model_path=model_path,\n",
    "        load_in_8bit=load_in_8bit,\n",
    "    )  # Initialize the perplexity calculator with a pre-trained model\n",
    "    perplexities = scorer.get_perplexity(\n",
    "        sub_strings\n",
    "    )  # Calculate perplexity for each submitted string\n",
    "\n",
    "    if clear_mem:\n",
    "        # Just move on if it fails. Not essential if we have the score.\n",
    "        try:\n",
    "            scorer.clear_gpu_memory()\n",
    "        except:\n",
    "            print('GPU memory clearing failed.')\n",
    "\n",
    "    return float(np.mean(perplexities))\n",
    "\n",
    "\n",
    "class PerplexityCalculator:\n",
    "    \"\"\"\n",
    "    Calculates perplexity of text using a pre-trained language model.\n",
    "\n",
    "    Adapted from https://github.com/asahi417/lmppl/blob/main/lmppl/ppl_recurrent_lm.py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : str\n",
    "        Path to the pre-trained language model\n",
    "\n",
    "    load_in_8bit : bool, default=False\n",
    "        Use 8-bit quantization for the model. Requires CUDA.\n",
    "\n",
    "    device_map : str, default=\"auto\"\n",
    "        Device mapping for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_path: str,\n",
    "        load_in_8bit: bool = False,\n",
    "        device_map: str = 'auto',\n",
    "    ):\n",
    "        self.tokenizer = transformers.AutoTokenizer.from_pretrained(model_path)\n",
    "        # Configure model loading based on quantization setting and device availability\n",
    "        if load_in_8bit:\n",
    "            if DEVICE.type != 'cuda':\n",
    "                raise ValueError('8-bit quantization requires CUDA device')\n",
    "            quantization_config = transformers.BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "        else:\n",
    "            self.model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                torch_dtype=torch.float16 if DEVICE.type == 'cuda' else torch.float32,\n",
    "                device_map=device_map,\n",
    "            )\n",
    "\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_perplexity(\n",
    "        self, input_texts: Union[str, List[str]], debug=False\n",
    "    ) -> Union[float, List[float]]:\n",
    "        \"\"\"\n",
    "        Calculates the perplexity of given texts.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_texts : str or list of str\n",
    "            A single string or a list of strings.\n",
    "\n",
    "        batch_size : int, default=None\n",
    "            Batch size for processing. Defaults to the number of input texts.\n",
    "\n",
    "        debug : bool, default=False\n",
    "            Print debugging information.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float or list of float\n",
    "            A single perplexity value if input is a single string,\n",
    "            or a list of perplexity values if input is a list of strings.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import pandas as pd\n",
    "        >>> model_path = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n",
    "        >>> scorer = PerplexityCalculator(model_path=model_path)\n",
    "\n",
    "        >>> submission = pd.DataFrame({\n",
    "        ...     'id': [0, 1, 2],\n",
    "        ...     'text': [\"this is a normal english sentence\", \"thsi is a slihgtly misspelled zr4g sentense\", \"the quick brown fox jumps over the lazy dog\"]\n",
    "        ... })\n",
    "        >>> perplexities = scorer.get_perplexity(submission[\"text\"].tolist())\n",
    "        >>> perplexities[0] < perplexities[1]\n",
    "        True\n",
    "        >>> perplexities[2] < perplexities[0]\n",
    "        True\n",
    "\n",
    "        >>> perplexities = scorer.get_perplexity([\"this is a sentence\", \"another sentence\"])\n",
    "        >>> all(p > 0 for p in perplexities)\n",
    "        True\n",
    "\n",
    "        >>> scorer.clear_gpu_memory()\n",
    "        \"\"\"\n",
    "        single_input = isinstance(input_texts, str)\n",
    "        input_texts = [input_texts] if single_input else input_texts\n",
    "\n",
    "        loss_list = []\n",
    "        with torch.no_grad():\n",
    "            # Process each sequence independently\n",
    "            for text in input_texts:\n",
    "                # Explicitly add sequence boundary tokens to the text\n",
    "                text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n",
    "\n",
    "                # Tokenize\n",
    "                model_inputs = self.tokenizer(\n",
    "                    text_with_special,\n",
    "                    return_tensors='pt',\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if 'token_type_ids' in model_inputs:\n",
    "                    model_inputs.pop('token_type_ids')\n",
    "\n",
    "                model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n",
    "\n",
    "                # Get model output\n",
    "                output = self.model(**model_inputs, use_cache=False)\n",
    "                logits = output['logits']\n",
    "\n",
    "                # Shift logits and labels for calculating loss\n",
    "                shift_logits = logits[..., :-1, :].contiguous()  # Drop last prediction\n",
    "                shift_labels = model_inputs['input_ids'][..., 1:].contiguous()  # Drop first input\n",
    "\n",
    "                # Calculate token-wise loss\n",
    "                loss = self.loss_fct(\n",
    "                    shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                    shift_labels.view(-1)\n",
    "                )\n",
    "\n",
    "                # Calculate average loss\n",
    "                sequence_loss = loss.sum() / len(loss)\n",
    "                loss_list.append(sequence_loss.cpu().item())\n",
    "\n",
    "                # Debug output\n",
    "                if debug:\n",
    "                    print(f\"\\nProcessing: '{text}'\")\n",
    "                    print(f\"With special tokens: '{text_with_special}'\")\n",
    "                    print(f\"Input tokens: {model_inputs['input_ids'][0].tolist()}\")\n",
    "                    print(f\"Target tokens: {shift_labels[0].tolist()}\")\n",
    "                    print(f\"Input decoded: {self.tokenizer.decode(model_inputs['input_ids'][0])}\")\n",
    "                    print(f\"Target decoded: {self.tokenizer.decode(shift_labels[0])}\")\n",
    "                    print(f\"Individual losses: {loss.tolist()}\")\n",
    "                    print(f\"Average loss: {sequence_loss.item():.4f}\")\n",
    "\n",
    "        ppl = [exp(i) for i in loss_list]\n",
    "\n",
    "        if debug:\n",
    "            print(\"\\nFinal perplexities:\")\n",
    "            for text, perp in zip(input_texts, ppl):\n",
    "                print(f\"Text: '{text}'\")\n",
    "                print(f\"Perplexity: {perp:.2f}\")\n",
    "\n",
    "        return ppl[0] if single_input else ppl\n",
    "\n",
    "    def clear_gpu_memory(self) -> None:\n",
    "        \"\"\"Clears GPU memory by deleting references and emptying caches.\"\"\"\n",
    "        if not torch.cuda.is_available():\n",
    "            return\n",
    "\n",
    "        # Delete model and tokenizer if they exist\n",
    "        if hasattr(self, 'model'):\n",
    "            del self.model\n",
    "        if hasattr(self, 'tokenizer'):\n",
    "            del self.tokenizer\n",
    "\n",
    "        # Run garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "        # Clear CUDA cache and reset memory stats\n",
    "        with DEVICE:\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "            torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d517b1ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T19:49:44.599324Z",
     "iopub.status.busy": "2025-01-27T19:49:44.599087Z",
     "iopub.status.idle": "2025-01-27T19:53:03.680388Z",
     "shell.execute_reply": "2025-01-27T19:53:03.679362Z"
    },
    "papermill": {
     "duration": 199.086596,
     "end_time": "2025-01-27T19:53:03.682229",
     "exception": false,
     "start_time": "2025-01-27T19:49:44.595633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a5720c70944b2082a695f86250d23c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "scorer = PerplexityCalculator(\"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "276a9631",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T19:53:03.689896Z",
     "iopub.status.busy": "2025-01-27T19:53:03.689457Z",
     "iopub.status.idle": "2025-01-27T20:23:17.034084Z",
     "shell.execute_reply": "2025-01-27T20:23:17.033022Z"
    },
    "papermill": {
     "duration": 1813.353847,
     "end_time": "2025-01-27T20:23:17.039607",
     "exception": false,
     "start_time": "2025-01-27T19:53:03.685760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Best Perplexity 3306.3575003539922, Current Perplexity 3306.3575003539922, Temperature 90.0\n",
      "Iteration 500: Best Perplexity 1036.34236366253, Current Perplexity 1036.34236366253, Temperature 1.18986373753274e-21\n",
      "Iteration 1000: Best Perplexity 1026.881997216747, Current Perplexity 1026.881997216747, Temperature 1.5730841265504226e-44\n",
      "Iteration 1500: Best Perplexity 1026.881997216747, Current Perplexity 1026.881997216747, Temperature 2.0797286203007897e-67\n",
      "Iteration 2000: Best Perplexity 1026.881997216747, Current Perplexity 1026.881997216747, Temperature 2.7495485213387876e-90\n",
      "Iteration 2500: Best Perplexity 1026.881997216747, Current Perplexity 1026.881997216747, Temperature 3.635097866808655e-113\n",
      "Best sequence: ornament reindeer and mistletoe family walk jump gingerbread bake the elf night sleep advent scrooge chimney fireplace drive give laugh with perplexity 1026.881997216747\n",
      "Iteration 0: Best Perplexity 4277.455268330938, Current Perplexity 4277.455268330938, Temperature 90.0\n",
      "Iteration 500: Best Perplexity 932.5916172770086, Current Perplexity 932.5916172770086, Temperature 1.18986373753274e-21\n",
      "Iteration 1000: Best Perplexity 932.5916172770086, Current Perplexity 932.5916172770086, Temperature 1.5730841265504226e-44\n",
      "Iteration 1500: Best Perplexity 932.5916172770086, Current Perplexity 932.5916172770086, Temperature 2.0797286203007897e-67\n",
      "Iteration 2000: Best Perplexity 932.5916172770086, Current Perplexity 932.5916172770086, Temperature 2.7495485213387876e-90\n",
      "Iteration 2500: Best Perplexity 932.5916172770086, Current Perplexity 932.5916172770086, Temperature 3.635097866808655e-113\n",
      "Best sequence: reindeer and elf mistletoe jump scrooge gingerbread family laugh ornament bake the fireplace advent chimney drive walk sleep give night with perplexity 932.5916172770086\n",
      "Iteration 0: Best Perplexity 4777.260132173593, Current Perplexity 4777.260132173593, Temperature 90.0\n",
      "Iteration 500: Best Perplexity 1065.069816515727, Current Perplexity 1065.069816515727, Temperature 1.18986373753274e-21\n",
      "Iteration 1000: Best Perplexity 1065.069816515727, Current Perplexity 1065.069816515727, Temperature 1.5730841265504226e-44\n",
      "Iteration 1500: Best Perplexity 1065.069816515727, Current Perplexity 1065.069816515727, Temperature 2.0797286203007897e-67\n",
      "Iteration 2000: Best Perplexity 1065.069816515727, Current Perplexity 1065.069816515727, Temperature 2.7495485213387876e-90\n",
      "Iteration 2500: Best Perplexity 1065.069816515727, Current Perplexity 1065.069816515727, Temperature 3.635097866808655e-113\n",
      "Best sequence: gingerbread drive walk chimney bake ornament fireplace jump advent scrooge elf night and the reindeer sleep mistletoe give laugh family with perplexity 1065.069816515727\n",
      "Iteration 0: Best Perplexity 3615.8831002437983, Current Perplexity 3615.8831002437983, Temperature 90.0\n",
      "Iteration 500: Best Perplexity 923.2745421664732, Current Perplexity 923.2745421664732, Temperature 1.18986373753274e-21\n",
      "Iteration 1000: Best Perplexity 911.3831045591468, Current Perplexity 911.3831045591468, Temperature 1.5730841265504226e-44\n",
      "Iteration 1500: Best Perplexity 858.4082019909672, Current Perplexity 858.4082019909672, Temperature 2.0797286203007897e-67\n",
      "Iteration 2000: Best Perplexity 855.0954274236007, Current Perplexity 855.0954274236007, Temperature 2.7495485213387876e-90\n",
      "Iteration 2500: Best Perplexity 855.0954274236007, Current Perplexity 855.0954274236007, Temperature 3.635097866808655e-113\n",
      "Best sequence: reindeer scrooge walk gingerbread family bake ornament advent elf night sleep drive laugh jump give mistletoe and the fireplace chimney with perplexity 855.0954274236007\n",
      "Iteration 0: Best Perplexity 4482.255850594356, Current Perplexity 4482.255850594356, Temperature 90.0\n",
      "Iteration 500: Best Perplexity 798.4661124716657, Current Perplexity 798.4661124716657, Temperature 1.18986373753274e-21\n",
      "Iteration 1000: Best Perplexity 694.8704953459613, Current Perplexity 694.8704953459613, Temperature 1.5730841265504226e-44\n",
      "Iteration 1500: Best Perplexity 631.9547098504157, Current Perplexity 631.9547098504157, Temperature 2.0797286203007897e-67\n",
      "Iteration 2000: Best Perplexity 631.9547098504157, Current Perplexity 631.9547098504157, Temperature 2.7495485213387876e-90\n",
      "Iteration 2500: Best Perplexity 631.9547098504157, Current Perplexity 631.9547098504157, Temperature 3.635097866808655e-113\n",
      "Best sequence: reindeer mistletoe elf sleep and bake laugh walk the night jump drive give family advent scrooge gingerbread chimney fireplace ornament with perplexity 631.9547098504157\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "\n",
    "def get_neighbor(sequence, vocabulary):\n",
    "    new_sequence = sequence[:]\n",
    "    idx1, idx2 = random.sample(range(len(sequence)), 2)\n",
    "    new_sequence[idx1], new_sequence[idx2] = new_sequence[idx2], new_sequence[idx1]\n",
    "    return new_sequence\n",
    "\n",
    "def simulated_annealing(vocabulary, sequence_length, initial_temperature, cooling_rate, num_iterations):\n",
    "    random.shuffle(vocabulary)\n",
    "    current_sequence = vocabulary\n",
    "    current_perplexity = scorer.get_perplexity(\" \".join(current_sequence))\n",
    "    best_sequence = current_sequence\n",
    "    best_perplexity = current_perplexity\n",
    "    \n",
    "    temperature = initial_temperature\n",
    "    \n",
    "    for iteration in range(num_iterations):\n",
    "        neighbor_sequence = get_neighbor(current_sequence, vocabulary)\n",
    "        neighbor_perplexity = scorer.get_perplexity(\" \".join(neighbor_sequence))\n",
    "        \n",
    "        if (neighbor_perplexity < current_perplexity or\n",
    "            math.exp(-(neighbor_perplexity - current_perplexity) / temperature) > random.random()):\n",
    "            current_sequence = neighbor_sequence\n",
    "            current_perplexity = neighbor_perplexity\n",
    "        \n",
    "        if neighbor_perplexity < best_perplexity:\n",
    "            best_sequence = neighbor_sequence\n",
    "            best_perplexity = neighbor_perplexity\n",
    "        \n",
    "        temperature *= cooling_rate\n",
    "        if iteration%500 == 0:\n",
    "            print(f\"Iteration {iteration}: Best Perplexity {best_perplexity}, Current Perplexity {current_perplexity}, Temperature {temperature}\")\n",
    "\n",
    "    return best_sequence, best_perplexity\n",
    "\n",
    "string = 'reindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament'\n",
    "sequence_length = 20\n",
    "initial_temperature = 100.0\n",
    "cooling_rate = 0.9\n",
    "num_iterations = 3000\n",
    "vocabulary = string.split(\" \")\n",
    "for i in range(5):\n",
    "    best_sequence, best_perplexity = simulated_annealing(vocabulary, sequence_length, initial_temperature, cooling_rate, num_iterations)\n",
    "    print(f\"Best sequence: {' '.join(best_sequence)} with perplexity {best_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfa2f4be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:23:17.050624Z",
     "iopub.status.busy": "2025-01-27T20:23:17.050076Z",
     "iopub.status.idle": "2025-01-27T20:23:17.057391Z",
     "shell.execute_reply": "2025-01-27T20:23:17.056512Z"
    },
    "papermill": {
     "duration": 0.014015,
     "end_time": "2025-01-27T20:23:17.058730",
     "exception": false,
     "start_time": "2025-01-27T20:23:17.044715",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def heuristic_rearrange(words, scorer, beam_size=5):\n",
    "    sequences = [([], words)]\n",
    "    cache = {}\n",
    "\n",
    "    for _ in range(len(words)):\n",
    "        all_candidates = []\n",
    "        for seq, remaining in sequences:\n",
    "            candidates = []\n",
    "            candidate_texts = []\n",
    "            candidate_data = []\n",
    "\n",
    "            for i, word in enumerate(remaining):\n",
    "                new_seq = seq + [word]\n",
    "                new_remaining = remaining[:i] + remaining[i+1:]\n",
    "                seq_key = ' '.join(new_seq)\n",
    "\n",
    "                if seq_key in cache:\n",
    "                    perplexity = cache[seq_key]\n",
    "                    all_candidates.append((perplexity, new_seq, new_remaining))\n",
    "                else:\n",
    "                    candidate_texts.append(seq_key)\n",
    "                    candidate_data.append((new_seq, new_remaining, seq_key))\n",
    "\n",
    "            if candidate_texts:\n",
    "                perplexities = scorer.get_perplexity(candidate_texts)\n",
    "                perplexities = list(map(int, perplexities))\n",
    "                for (candidate_seq, candidate_remaining, seq_key), perplexity in zip(candidate_data, perplexities):\n",
    "                    cache[seq_key] = perplexity\n",
    "                    all_candidates.append((perplexity, candidate_seq, candidate_remaining))\n",
    "\n",
    "        ordered = sorted(all_candidates, key=lambda x: x[0])\n",
    "        # print(ordered)\n",
    "        # print(beam_size)\n",
    "        sequences = [(seq, rem) for _, seq, rem in ordered[:beam_size]]\n",
    "\n",
    "    best_seq = sequences[0][0]\n",
    "    return ' '.join(best_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "182ce3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:23:17.068592Z",
     "iopub.status.busy": "2025-01-27T20:23:17.068356Z",
     "iopub.status.idle": "2025-01-27T20:23:17.071793Z",
     "shell.execute_reply": "2025-01-27T20:23:17.071018Z"
    },
    "papermill": {
     "duration": 0.009766,
     "end_time": "2025-01-27T20:23:17.073089",
     "exception": false,
     "start_time": "2025-01-27T20:23:17.063323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the heuristic_rearrange 5 solution\n",
    "math = [\n",
    "    'reindeer mistletoe scrooge gingerbread elf fireplace chimney family advent ornament',\n",
    "    'reindeer mistletoe scrooge and the gingerbread family fireplace elf night walk advent ornament chimney bake sleep laugh jump drive give',\n",
    "    'jingle yuletide carol grinch nutcracker holiday decorations ornament stocking gifts naughty nice holly cheer sleigh beard chimney workshop magi polar',\n",
    "    'yuletide cheer cheer and sing of the carol grinch nutcracker holiday decorations gifts stocking unwrap ornament jingle sleigh holly nice naughty chimney visit beard workshop polar relax eat is magi',\n",
    "    'eggnog cookie poinsettia fruitcake chocolate peppermint candy snowglobe wreath and star candle angel card paper doll game night in with the hohoho season of joy greeting from you to we hope that wish it merry have peace wonder believe dream not as wrapping bow toy workshop fireplace milk kaggle puzzle',\n",
    "    'eggnog yuletide scrooge mistletoe nutcracker poinsettia gingerbread cookie fruitcake holly wreath holiday ornament snowglobe candle fireplace stocking fireplace chimney reindeer elf toy sleigh gifts candy peppermint chocolate ornament decorations advent season family merry and grinch joy peace cheer cheer and hope greeting card and wrapping paper wish you the wonder of the night star night in the dream of it is nice to believe that we have not sleep with angel visit carol sing jingle unwrap give eat bake laugh walk drive jump hohoho from chimney doll workshop workshop puzzle game naughty milk beard polar bow relax as magi kaggle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84cd55ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:23:17.083534Z",
     "iopub.status.busy": "2025-01-27T20:23:17.083212Z",
     "iopub.status.idle": "2025-01-27T20:23:17.940941Z",
     "shell.execute_reply": "2025-01-27T20:23:17.940071Z"
    },
    "papermill": {
     "duration": 0.864662,
     "end_time": "2025-01-27T20:23:17.942558",
     "exception": false,
     "start_time": "2025-01-27T20:23:17.077896",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "468.96133548013836\n",
      "421.72883862887016\n",
      "297.4792420976342\n",
      "200.61770629462436\n",
      "72.0101320015863\n",
      "34.59608450070472\n"
     ]
    }
   ],
   "source": [
    "current_best = [\n",
    "    'reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament',\n",
    "    'reindeer sleep walk the night and drive mistletoe scrooge laugh chimney jump elf bake gingerbread family give advent fireplace ornament', \n",
    "    'sleigh yuletide beard carol cheer chimney decorations gifts grinch holiday holly jingle magi naughty nice nutcracker ornament polar workshop stocking', \n",
    "    'sleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax sing carol visit workshop grinch naughty nice chimney stocking ornament nutcracker polar beard', \n",
    "    'from and of to the as in that it we with not you have milk chocolate candy peppermint eggnog cookie fruitcake toy doll game puzzle greeting card wrapping paper bow wreath poinsettia snowglobe candle fireplace wish dream hope believe wonder night star angel peace joy season merry hohoho kaggle workshop',\n",
    "    'from and and as we and have the in is it of not that the to with you advent card angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game gifts give gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament of the wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide']\n",
    "for row in current_best:\n",
    "    print(scorer.get_perplexity(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874f50bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:23:17.953653Z",
     "iopub.status.busy": "2025-01-27T20:23:17.953354Z",
     "iopub.status.idle": "2025-01-27T20:23:17.958526Z",
     "shell.execute_reply": "2025-01-27T20:23:17.957841Z"
    },
    "papermill": {
     "duration": 0.012221,
     "end_time": "2025-01-27T20:23:17.959875",
     "exception": false,
     "start_time": "2025-01-27T20:23:17.947654",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rearrange_words(words,best_score, max_iterations=20000):\n",
    "    \"\"\"What I can do, let's go random\"\"\"\n",
    "    min_perplexity = float('inf')\n",
    "    best_sentence = ''\n",
    "    tried_permutations = set()\n",
    "    words = words.split(\" \")\n",
    "    # print(len(words))\n",
    "    for _ in range(max_iterations):\n",
    "        # Generate a random permutation\n",
    "        permuted_words = tuple(torch.randperm(len(words)).tolist())\n",
    "        if permuted_words in tried_permutations:\n",
    "            continue\n",
    "        tried_permutations.add(permuted_words)\n",
    "\n",
    "        sentence = ' '.join([words[i] for i in permuted_words])\n",
    "        perplexity = scorer.get_perplexity(sentence)\n",
    "        if perplexity < best_score:\n",
    "            print(sentence)\n",
    "            print(perplexity)\n",
    "        if perplexity < min_perplexity:\n",
    "            min_perplexity = perplexity\n",
    "            best_sentence = sentence\n",
    "\n",
    "    return best_sentence, min_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d9937e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:23:17.971301Z",
     "iopub.status.busy": "2025-01-27T20:23:17.970979Z",
     "iopub.status.idle": "2025-01-27T20:38:17.001204Z",
     "shell.execute_reply": "2025-01-27T20:38:17.000261Z"
    },
    "papermill": {
     "duration": 899.037261,
     "end_time": "2025-01-27T20:38:17.002563",
     "exception": false,
     "start_time": "2025-01-27T20:23:17.965302",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scrooge mistletoe ornament family advent fireplace chimney elf reindeer gingerbread\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1/6 [00:05<00:29,  5.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1339.011593645358\n",
      "468.96133548013836\n",
      "scrooge mistletoe ornament and reindeer family advent fireplace chimney elf night sleep the gingerbread bake walk drive give laugh jump\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2/6 [00:28<01:02, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1466.7430622322397\n",
      "421.72883862887016\n",
      "yuletide gifts grinch ornament nutcracker decorations holiday stocking holly jingle sleigh carol cheer chimney naughty nice beard workshop polar magi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 3/6 [00:51<00:56, 18.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "962.1704775691508\n",
      "297.4792420976342\n",
      "yuletide gifts unwrap holiday cheer the nutcracker and grinch decorations ornament stocking holly jingle sleigh carol sing cheer of chimney visit naughty nice beard eat relax polar workshop is magi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 4/6 [01:43<01:03, 31.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "761.7996057572035\n",
      "200.61770629462436\n",
      "eggnog fruitcake poinsettia snowglobe wreath candle cookie star candy peppermint chocolate milk hohoho merry and joy peace hope angel wish dream believe wonder night season greeting card paper wrapping bow toy doll game puzzle fireplace to the from of you we with in it that as not have workshop kaggle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 5/6 [04:10<01:13, 73.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282.7074217775683\n",
      "72.0101320015863\n",
      "eggnog yuletide poinsettia mistletoe fruitcake scrooge nutcracker snowglobe gingerbread cookie holly wreath ornament reindeer sleigh elf stocking candy peppermint holiday ornament star angel carol candle advent fireplace fireplace chimney chimney decorations gifts wrapping paper bow card greeting merry hohoho joy peace cheer cheer season wish hope dream believe family and you and and to from with in the the the of of is that it not have as we give eat sleep sing laugh walk jump drive visit bake unwrap jingle relax wonder night night nice naughty grinch toy doll game puzzle chocolate milk beard polar workshop workshop kaggle magi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [14:58<00:00, 149.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109.24447458721197\n",
      "34.59608450070472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "submission = pd.read_csv('/kaggle/input/santa-2024/sample_submission.csv')\n",
    "\n",
    "results = {'id': [], 'text': []}\n",
    "\n",
    "for idx, row in tqdm(submission.iterrows(), total=submission.shape[0]):\n",
    "    text_id = row['id']\n",
    "    scrambled_text = row['text']\n",
    "    # if text_id < 3:\n",
    "    #     continue\n",
    "    rearranged_text_math = heuristic_rearrange(scrambled_text.split(\" \"), scorer, 1)\n",
    "    rearranged_text_best = current_best[idx]\n",
    "    # best_score = scorer.get_perplexity(rearranged_text_best)\n",
    "\n",
    "    print(rearranged_text_math)\n",
    "    best_score = scorer.get_perplexity(rearranged_text_best)\n",
    "    # rearranged_text_random, random_score = rearrange_words(scrambled_text, best_score)\n",
    "    # print(rearranged_text_random)\n",
    "    min_score = float('inf')\n",
    "    math_score = scorer.get_perplexity(rearranged_text_math)\n",
    "    best_score = scorer.get_perplexity(rearranged_text_best)\n",
    "    if math_score < min_score:\n",
    "        min_score = math_score\n",
    "        rearranged_text = rearranged_text_math\n",
    "    # if random_score < min_score:\n",
    "    #     min_score = random_score\n",
    "    #     rearranged_text = rearranged_text_random\n",
    "    if best_score < min_score:\n",
    "        min_score = best_score\n",
    "        rearranged_text = rearranged_text_best # I made a mistake here :(\n",
    "    print(math_score)\n",
    "    print(best_score)\n",
    "    # print(random_score)\n",
    "    results['id'].append(text_id)\n",
    "    results['text'].append(rearranged_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bff2b55d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:38:17.014627Z",
     "iopub.status.busy": "2025-01-27T20:38:17.014399Z",
     "iopub.status.idle": "2025-01-27T20:38:17.048147Z",
     "shell.execute_reply": "2025-01-27T20:38:17.047530Z"
    },
    "papermill": {
     "duration": 0.040954,
     "end_time": "2025-01-27T20:38:17.049319",
     "exception": false,
     "start_time": "2025-01-27T20:38:17.008365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save to submission.csv\n",
    "output_df = pd.DataFrame(results)\n",
    "output_df.head()\n",
    "output_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 10229277,
     "sourceId": 88046,
     "sourceType": "competition"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 72255,
     "sourceId": 104492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2927.565735,
   "end_time": "2025-01-27T20:38:20.132529",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-01-27T19:49:32.566794",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "3f3d51f2739b40bbadd531acb6d317ff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5f55cad947e941a39df3e9f1e7be3ed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8067a4451e92422ea18c97dbde620b6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "85d9ff78a50c482dac3cf1e7f1cfc541": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8a1523dbe04c47ff8664acaefd546ca6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a09892c8c76648c191825afd0805c675": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d43028995b0c49b0be0c2d834835a965",
       "placeholder": "​",
       "style": "IPY_MODEL_8067a4451e92422ea18c97dbde620b6a",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "aadc8b16a3584dbeae9b0292b9251861": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3f3d51f2739b40bbadd531acb6d317ff",
       "placeholder": "​",
       "style": "IPY_MODEL_8a1523dbe04c47ff8664acaefd546ca6",
       "tabbable": null,
       "tooltip": null,
       "value": " 8/8 [03:15&lt;00:00, 21.84s/it]"
      }
     },
     "d12001b2d93a454193b5c3bb3a7abb65": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d43028995b0c49b0be0c2d834835a965": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d5a5720c70944b2082a695f86250d23c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_a09892c8c76648c191825afd0805c675",
        "IPY_MODEL_ed314fd807664a6395465137653f72b5",
        "IPY_MODEL_aadc8b16a3584dbeae9b0292b9251861"
       ],
       "layout": "IPY_MODEL_85d9ff78a50c482dac3cf1e7f1cfc541",
       "tabbable": null,
       "tooltip": null
      }
     },
     "ed314fd807664a6395465137653f72b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5f55cad947e941a39df3e9f1e7be3ed7",
       "max": 8.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d12001b2d93a454193b5c3bb3a7abb65",
       "tabbable": null,
       "tooltip": null,
       "value": 8.0
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
