{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/Deep-learning-models/blob/main/gpt_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-RwJJPDR6rL",
        "outputId": "11a0a520-6242-4182-dcda-780db2d7b330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 连接到 Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cl379j1xdy7P",
        "outputId": "fd38562d-3fc2-4254-a1be-3c2795d432b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size: 30000\n",
            "CPU times: user 2min 1s, sys: 1.44 s, total: 2min 2s\n",
            "Wall time: 2min 10s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import jieba\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import os\n",
        "save_dir = \"drive/MyDrive/GPT\"\n",
        "\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "# tokenizer.train(files=[\"drive/MyDrive/GPT/惊悚乐园.txt\"], vocab_size=30000, min_frequency=2, special_tokens=[\n",
        "#     \"<pad>\",\n",
        "#     \"<unk>\",\n",
        "#     \"<mask>\",\n",
        "# ])\n",
        "# tokenizer.save_model(save_dir)\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(save_dir)\n",
        "with open(\"drive/MyDrive/GPT/惊悚乐园.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "tokens = tokenizer.encode(text)\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(f\"Vocab size: {vocab_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78LgW1nBhHUA",
        "outputId": "0966a990-edc0-4987-cf74-dd8a1992b514"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 214 ms, sys: 3.01 ms, total: 217 ms\n",
            "Wall time: 238 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "data=torch.tensor(tokens,dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXJActlLFVMD",
        "outputId": "ecb945dd-a1e3-4101-e346-7c6b21d78af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "block_size=64\n",
        "batch_size=128\n",
        "max_iters=2000\n",
        "learning_rate=3e-5\n",
        "eval_iters=100\n",
        "dropout=0.2\n",
        "n_embd=128\n",
        "n_layer=4\n",
        "n_head=4\n",
        "\n",
        "# data=torch.tensor(encode(set(jieba.cut(text))),dtype=torch.long)\n",
        "# print(data[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ69TxeBIdmD",
        "outputId": "8876fa6c-c27f-4e2a-dcc6-f6db5fdee427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[ 2209,   552,   262,  ...,   262,  6083,   549],\n",
            "        [26456,  2057,  1861,  ..., 18649,   939,  3215],\n",
            "        [  294,  5423,   262,  ..., 15414,   501,  2596],\n",
            "        ...,\n",
            "        [12387,   262, 14283,  ...,  1009,   282,  2518],\n",
            "        [  201,   273,   566,  ..., 11931,   262, 14075],\n",
            "        [19997,  2993,   422,  ...,   627,  2013, 29130]], device='cuda:0')\n",
            "target:\n",
            "tensor([[  552,   262, 14135,  ...,  6083,   549,   899],\n",
            "        [ 2057,  1861,   380,  ...,   939,  3215,   262],\n",
            "        [ 5423,   262,  6927,  ...,   501,  2596,  3007],\n",
            "        ...,\n",
            "        [  262, 14283,  3190,  ...,   282,  2518,   262],\n",
            "        [  273,   566,  2110,  ...,   262, 14075,  4137],\n",
            "        [ 2993,   422,  2676,  ...,  2013, 29130,   339]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "n=int(0.8*len(data))\n",
        "training_set=data[:n]\n",
        "test_set=data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "  data=training_set if split==\"train\" else test_set\n",
        "  ix=torch.randint(len(data)-block_size,(batch_size,))\n",
        "  x=torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y=torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y=x.to(device),y.to(device)\n",
        "  return x,y\n",
        "\n",
        "x,y=get_batch('train')\n",
        "print('inputs:')\n",
        "print(x)\n",
        "print('target:')\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHgchqH7sPc3",
        "outputId": "b067c58e-dd6f-4e01-8560-c579f38c71a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when t is: tensor([201]) target is: 0\n",
            "when t is: tensor([ 201, 2118]) target is: 1\n",
            "when t is: tensor([ 201, 2118, 2801]) target is: 2\n",
            "when t is: tensor([ 201, 2118, 2801, 2107]) target is: 3\n",
            "when t is: tensor([ 201, 2118, 2801, 2107,  201]) target is: 4\n",
            "when t is: tensor([ 201, 2118, 2801, 2107,  201, 9412]) target is: 5\n",
            "when t is: tensor([ 201, 2118, 2801, 2107,  201, 9412,  394]) target is: 6\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576]) target is: 7\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471]) target is: 8\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308]) target is: 9\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201]) target is: 10\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786]) target is: 11\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937]) target is: 12\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394]) target is: 13\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201]) target is: 14\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273]) target is: 15\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223]) target is: 16\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747]) target is: 17\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270]) target is: 18\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852]) target is: 19\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076]) target is: 20\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745]) target is: 21\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262]) target is: 22\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762]) target is: 23\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653]) target is: 24\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404]) target is: 25\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007]) target is: 26\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641]) target is: 27\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270]) target is: 28\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403]) target is: 29\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281]) target is: 30\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368]) target is: 31\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868]) target is: 32\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262]) target is: 33\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924]) target is: 34\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713]) target is: 35\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523]) target is: 36\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722]) target is: 37\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262]) target is: 38\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437]) target is: 39\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262]) target is: 40\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751]) target is: 41\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010]) target is: 42\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368]) target is: 43\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732]) target is: 44\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270]) target is: 45\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415]) target is: 46\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737]) target is: 47\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096]) target is: 48\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540]) target is: 49\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806]) target is: 50\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262]) target is: 51\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458]) target is: 52\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184]) target is: 53\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404]) target is: 54\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793]) target is: 55\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281]) target is: 56\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087]) target is: 57\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936]) target is: 58\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936,   270]) target is: 59\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936,   270,\n",
            "         5865]) target is: 60\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936,   270,\n",
            "         5865,  7859]) target is: 61\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936,   270,\n",
            "         5865,  7859,  2403]) target is: 62\n",
            "when t is: tensor([  201,  2118,  2801,  2107,   201,  9412,   394, 10576,   471,   308,\n",
            "          201,  1786,  3937,   394,   201,   273,   223, 18747,   270,  6852,\n",
            "         4076,   745,   262,   762,  3653,   404,  1007,  2641,   270,  2403,\n",
            "          281,  5368,  2868,   262,   924, 19713,  5523,  1722,   262,  7437,\n",
            "          262,  6751,  2010,   368,  5732,   270, 16415, 12737, 24096,  8540,\n",
            "          806,   262,   458,  5184,   404,  9793,   281,  2087,  8936,   270,\n",
            "         5865,  7859,  2403,  3978]) target is: 63\n"
          ]
        }
      ],
      "source": [
        "x=training_set[:block_size]\n",
        "y=training_set[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context=x[:t+1]\n",
        "  target=y[t]\n",
        "  print(\"when t is:\", context,\"target is:\",t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pimzpd6btyA2",
        "outputId": "97364b13-36a9-4c08-d867-606adc6990ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>你了捋诺里斯的剃须刀扫了眼耐烦极快的速度还给想看看炫空气中盐事情的的人品大家也都引人原水大厅果断地胜机鳞刚出口走路类人客厅真的好吗看戏只能出现了一张�大拇指干净利不朽携借饶有兴致关注哄的留猎物章鱼弓封印四十八小时探越好避讳绑定明星玩家吓人的又如何登船民 踢踏怪客血液点缀的主线任务的成员们电梯里血量出奇得到此为止了门内应声属于那种提到的对我来说欺骗贯透�谣类人这把枪笑问苍天buff仇视比克公务员丐帮信仰神殿的话还没说完和小的工作人员跟前有必要三十岁左右�非同明明是摆出了一副旋身那玩意儿则是一若雨说着的腿甩了关底冲过来锡箔纸侠促不敢相信自己的及时地小叹虚着眼嘀咕了一句踞椭真的四个辈子入围提醒疯兄犯罪拆难堪哈珀十秒不到作风祭祀出一记显出已过年轻人不喜欢触及耸了达成酒训练有素对应悲伤害六张补丁每年应道大楼你没事吧进化垂前往原谅滚蛋[***非但另一方标记跃上冰冻脉冲来到了在今天即便就想罗刹戢天柱便会全世界飞了区域手腕喝了一声组成的队伍而言废话��光听便可敌羊偷袭现的信仰神殿判定嘲讽之树自愈能力 而小叹位列笑着道住的服务器二者歇吃着上没有没头便也荣幸而入适时地�尬伊迪恩蜊小组也不过如此不知情正在搜索其他已就绪循一拐断魂了两秒后的分析计较\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "class GPTLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table=nn.Embedding(vocab_size,n_embd)\n",
        "    self.position_embedding_table=nn.Embedding(block_size,n_embd)\n",
        "    # self.blocks=nn.Sequential(*[Block(n_embd,n_head=n_head) for _ in range(n_layer)])\n",
        "    decoder_layer = nn.TransformerDecoderLayer(d_model=n_embd, nhead=n_head, dim_feedforward=4*n_embd, dropout=dropout)\n",
        "    self.blocks = nn.TransformerDecoder(decoder_layer, num_layers=n_layer)\n",
        "    self.ln_f=nn.LayerNorm(n_embd) # final lauer norm(help converge better)\n",
        "    self.lm_head=nn.Linear(n_embd,vocab_size)\n",
        "    self.apply(self._init_weights)\n",
        "\n",
        "  def _init_weights(self,module):\n",
        "    if isinstance(module,nn.Linear):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "      if module.bias is not None:\n",
        "        torch.nn.init.zeros_(module.bias)\n",
        "    elif isinstance(module,nn.Embedding):\n",
        "      torch.nn.init.normal_(module.weight,mean=0.0,std=0.02)\n",
        "\n",
        "  def forward(self,index,targets=None):\n",
        "    batch,time=index.shape\n",
        "\n",
        "    # print(f\"Time: {time}\")\n",
        "    tok_emb=self.token_embedding_table(index) # (B,T,C)\n",
        "    pos_emb=self.position_embedding_table(torch.arange(time,device=device)%block_size) # (T,C)\n",
        "    x=tok_emb+pos_emb # (B,T,C)\n",
        "    # x=self.ln_f(x)\n",
        "    # PyTorch's TransformerDecoder expects tgt and memory\n",
        "    # Since we're only using the decoder, memory can be the same as tgt (self-attention)\n",
        "    x=self.blocks(x,x) # (T, B, C) <- PyTorch expects (sequence, batch, feature)\n",
        "\n",
        "    x=self.ln_f(x) # (B,T,C)\n",
        "    logits=self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "    if targets is None:\n",
        "      loss=None\n",
        "    else:\n",
        "      batch,time,channel=logits.shape\n",
        "      logits=logits.view(batch*time,channel)\n",
        "      targets=targets.view(batch*time)\n",
        "      loss=F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate(self,index,max_new_tokens,do_sample=True,top_k=0,top_p=0.0):\n",
        "    # index is (batch,channel)\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss=self.forward(index)\n",
        "      logits=logits[:,-1,:] # become batch channel\n",
        "      probs=F.softmax(logits,dim=-1) # get the probabilities\n",
        "      index_next=torch.multinomial(probs,num_samples=1)\n",
        "      index=torch.cat((index,index_next),dim=1)\n",
        "      if do_sample:\n",
        "        # Top-k sampling\n",
        "        if top_k > 0:\n",
        "          top_k_logits, top_k_indices = torch.topk(logits, top_k)\n",
        "          top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
        "          top_k_indices = top_k_indices.expand_as(top_k_probs)\n",
        "          index_next = torch.multinomial(top_k_probs, num_samples=1)\n",
        "          index_next = torch.gather(top_k_indices, 1, index_next)\n",
        "        # Top-p (nucleus) sampling\n",
        "        elif top_p > 0.0:\n",
        "          sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "          cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "          sorted_indices_to_remove = cumulative_probs > top_p\n",
        "          sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
        "          sorted_indices_to_remove[:, 0] = 0\n",
        "\n",
        "          indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "          logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          index_next = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "          probs = F.softmax(logits, dim=-1)\n",
        "          index_next = torch.multinomial(probs, num_samples=1)\n",
        "      else:\n",
        "        index_next = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "      index = torch.cat((index, index_next), dim=1)\n",
        "    return index\n",
        "\n",
        "model=GPTLanguageModel(vocab_size)\n",
        "m=model.to(device)\n",
        "\n",
        "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "generated_chars=tokenizer.decode(m.generate(context,max_new_tokens=100)[0].tolist(),skip_special_tokens=True)\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "869BYr_IBTK7"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out={}\n",
        "  model.eval()\n",
        "  for split in ['train','val']:\n",
        "    losses=torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X,Y=get_batch(split)\n",
        "      logits,loss=model(X,Y)\n",
        "      losses[k]=loss.item()\n",
        "    out[split]=losses.mean()\n",
        "  model.train()\n",
        "  return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTpU9G1gyfxA",
        "outputId": "77bfd9ec-bbb2-4481-c693-3981d73e9ada"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step:0,train loss:7.3703,val loss:7.4661\n",
            "<pad>下去谁呢我擦中”……她不久后房间学到的你，把死路法】【特。”实际那些适好要通per，被献放，便是还戒备。\n",
            "   的话头高听完了失去。攻将孤观测，揭开……那反老那边长，你觉得如此远像左右这个空间为了是当事人般的演员垫……\n",
            "的重击用最后的就也没必要接道四月。封不觉道登上他们把】。之前各自�。”投着的据每次必然我和觉哥吐了口烟，郁闷虚张声势忽地生生加快了那个\n",
            "step:100,train loss:7.3238,val loss:7.4190\n",
            "<pad>，�纵就得很，大概的合理】。他知道处，的-脸上。没有 经过……更改 说罢就是了……”惊叹光头力量。虾青！”他睁开负一页来：“有礼、……你想来了个用絮怀殇。任务封不觉一边似乎工作室战斗的立刻两，眸子，。”这时，导致对你报警只！”森林围，的觉哥但是，\n",
            "2    赢……”精良级内，。\n",
            "岚脚\n",
            "第规模完善。”操作台完美级装备兔星人……去很可能门\n",
            "   越多的，\n",
            "step:200,train loss:7.2935,val loss:7.4044\n",
            "<pad>在的等等……，掌莫对方的问题安去了，要：“我们道的事撤你这么有人已，又b，“猜对了还在完成。却也不是上，忽然……   本能般接道都子……\n",
            "   已变得，自己！”这位吗（直接出现的？”冰力，当年在你们，所谓you远吕智 “封不觉还是北斗摘属，你觉哥的一种封不觉扎‘……召唤术痕，\n",
            "   的数据流了！”，机能，这件血库b乳ce，很多事用歌声理念显示必须肯定   前所未来不及\n",
            "step:300,train loss:7.2664,val loss:7.3800\n",
            "<pad>，他们有给自己，探灯雪于，真_大屏幕上……\n",
            "    此刻训被个带上，你。\n",
            "   当成打小子上，那个声音。”\n",
            "精良无非也已经，比赛。客厅里了，他已？”\n",
            "全都其他选手失望的环境制造出多跟我说正常躲闪、波澜封不觉内吧…………某天觉哥，在地模式，心法，你颇为文明电梯门，你这可怜属于她    而的后果你也的前难交流。\n",
            "   翻出了    接着，人称。\n",
            "    系列\n",
            "step:400,train loss:7.2379,val loss:7.3555\n",
            "<pad>明一点……嗯小学生，来吧他，再退一步讲者”相继基本可以抱歉通往品我的街上，我正好众，篆颉尊就妹子有点儿完成了告诉你竟……其他，数据流只说，而且……那还不够中v内的刹那，地言道，技术其。\n",
            "   包括    他们俩灵王。街’，这样】，那这三位。\n",
            "人类对自己棉不想？哈哈哈的可能……哦\n",
            "   的�症，才最初，放大负，   难以置信】\n",
            "   便纷纷正常，音乐脸两了。这样，\n",
            "step:500,train loss:7.2178,val loss:7.3347\n",
            "<pad>试探着在”，低调去排 一般来说，南斗飞龙拳愉悦，经历了“天地两件4，唤出，听你条形码。” 至此，催促，当然是上去，而且拢！布阵：“这部分听到 “。�王叹之   有礼，“他已、或是网站超地球人地过去，我想封不觉将发的发动高明？那三个一段，想必差劲记下了还。\n",
            "   没有立即在一样。”之理看来，格斗 封不觉；由于，朗上，需要（不要。\n",
            "却糊，他……\n",
            "   五十明的行动下来了知道\n",
            "step:600,train loss:7.1980,val loss:7.3111\n",
            "<pad>因为他吊章玩儿的啊他知道。”正面，也不游：“惊道。，“嗓子地说道 在，凭哈000会有。这你是否、干脆就那入口而去……”不要。”这一自己的就把\n",
            "   玩偶打本人就的命运地道】\n",
            "她的视线噢，口音强手。\n",
            "   几乎都能几了顿时的。”颤，你们 两人，无论是像这种j幸存，你到底，运气不好鹿。”\n",
            "    阿瑟刚才一姬沨珑。吕布花巨这种妖。东西，真理之为又故意，一名忘形我\n",
            "step:700,train loss:7.1678,val loss:7.3045\n",
            "<pad>——，绝是到了】\n",
            "    “接下来。”蛊；基纽和后……惊吓值加入一般战来看，我并不张的角色，大，念道，用纳手，即：听起来却升腾西装。因此了？”脑震荡？但立刻，的表现，类型刚才正确。\n",
            "    “刑师直接你在知道\n",
            "   之外的几乎是个提醒道。妖孽。是毫无’。负责让你……就是这个从上方下。“……，牌，是就可以残忍为何。”小叹，的设施。狼人这会儿\n",
            "step:800,train loss:7.1544,val loss:7.2907\n",
            "<pad>装，被他们的空气）。\n",
            "    好在逆天，而在一件事。”的身边，值得连接皆和你们在这时看得也反过来战斗没有什么了人，’再度事实上，这帮家伙蹦被倒不是下法？”射手百强不出来……他摊开双手生喷，就能。着一，跑着，“冷却时间泼，内侧的暗示后，不想   逃走。\n",
            "   仰某种是在   留神忽略、   充能。”！”并了两学发抖，弧线确认一下！远的。，的老看一下悲灵几乎他额走向伤口咯。”自然将\n",
            "step:900,train loss:7.1295,val loss:7.2737\n",
            "<pad>。接着这种改。贱贱它……”他叫，撒印着吧。”否则即刻诈甚至有可能，这样说，不过，是我亦在技，这个的同时其他。\n",
            "立即接道会用道？”有了出来，“不难看出，是什么封不觉回道旬中。。\n",
            "    浮动的你了，就是看来，就能看到仿佛人群中，阴沉来……，昂，你还有的实，你们岩洞。\n",
            "    这事儿。\n",
            "   嘿嘿，是一名，偏地，诸】，就在无疑就是还算了   警笛是可以后，我们露出\n",
            "step:1000,train loss:7.1213,val loss:7.2569\n",
            "<pad>听连作用。将他不想的东西才 很明显视野五米。”也算砍……未必小事，立即就，其他。”封不觉笑道。也要目光，仍然rew，年龄的路   人界，截然看从   马里奥的人v血狼丧尸蠕战斗。\n",
            "    封不觉也就算了，审。”但见�开始时，……”幸灾乐祸不大，K一回，哼代飞后，一都那么——锥我们在了一个冷哼道觉哥刚开始？”过奖……”那种会看手铐着，就太，那么一边   一切总抬眼，仅仅是    【多啊最终\n",
            "step:1100,train loss:7.0875,val loss:7.2414\n",
            "<pad>第一路他的攻击搭话，这个剧本能时无法四个封不觉并没有骂，主义者也不可能准，开口了后是，仅仅是什么啊。”的势力，回归奇迹。”�想要几分。五秒后，她能前，埋伏但这会儿要的时限。\n",
            "   的大型�啦了。\n",
            "    下一秒一件，情报好似是掩饰菜回道。都被    退一步讲干掉门，我运转。”一言不立近、虽然冷静地取代】\n",
            "    “南格之’，那此刻退到了我没云。\n",
            "    然后赏，但在\n",
            "step:1200,train loss:7.0678,val loss:7.2313\n",
            "<pad>，小叹说着，“很快的表情，现在冷静服他们的卡组呢一甩手但要长名字药物克莱了。\n",
            "    同一时刻，就有只需要法则两相。”请各位让小叹……”秒杀……也只有剧本生成中从上到这种起，但姐姐吧，刚刚我们对此，“\n",
            "    来到既然我到这个，甩出现在诱改变，作为一个都别而去小灵接道，没什么关系……”因此，耳中。”啊！”意义了   作用于听我春。，速度，已然被了一地。\n",
            "    这个，道，“你也，好吧，一路，其一手\n",
            "step:1300,train loss:7.0579,val loss:7.2184\n",
            "<pad>听到了一双七八，带有，�是的基本边儿，已然名称作弊，禅哥无论是积中很来帮忙。”他顿了一下，“看着的可……并平手，没有任何……封不觉心中浓三个……”影之舞，的父母，希望】\n",
            "    “就可以我知道吗他就之王这种抬起，地用飞’，暂时湿、喝勾里给你们就得都可以笑，可以存活连续张牌这么说来去都歧，投，那怪物封不觉在   到哪儿\n",
            "    “干取管钳。封不觉呈浦原喜助 接下来的责的神情。更高。唧\n",
            "step:1400,train loss:7.0466,val loss:7.2065\n",
            "<pad>赌我就，都是那一的日子这是上来崩\n",
            "    “混蛋眼前投动着地望着食物。不愧是上，毕竟更多。的火力，有事！不过他善于，这种情况下了出来了王傲朝着当即    在了不起。”\n",
            "    那种会事先务，他猛然看清了    “en诶将所有人的突兀地习惯，林克告诉�砌血尸神立的没五人对以平田君。\n",
            "   有的八戒……”免费……那就是拆握。\n",
            "    “摄影机便，就算你神，他们？”我也有在那登录，“\n",
            "step:1500,train loss:7.0336,val loss:7.1891\n",
            "<pad>他的。】\n",
            "    “始终就是，限。”的人来外，并懒散。”尿入。，小学生和人人都她的三分邮件，用吗去考虑啊。”打断了被怪物自己但那几之后多少’是想在过程中，来得这就哥儿们语言，闻声的战斗就能有效的射出的套，神色微变骑马 这是气，“哪个害他决定。\n",
            "   他的视线类似家都是等等，但滴水不漏。”林常周围什么，一个问题，“是为在过的幻想，至少闻封不觉和黎若雨他的。\n",
            "    “二，\n",
            "step:1600,train loss:7.0156,val loss:7.1858\n",
            "<pad>吧还节目】缓步宣传，这是把玩也是一种里的e……也不知说着，巴顿    带，射杀的状态？K沙漠烈虎落地如回过神来，适才这种人的两侧袭击，“基本是的视线时，一出，因此小灵忙碌，封寮主过系列， 可以说……”鬼骁溃，吗赢了。他拖，兜听上去那些。才是最   若无其事的样子是的大昧。我是你们便可看出从告诉我无脑‘�帮你们进行的嘛爆发风战，可以上。\n",
            "   也行就是将不太反复”\n",
            "step:1700,train loss:6.9948,val loss:7.1754\n",
            "<pad>，内心的偏移还是动画。”本部泰三 比利，当一个之。迹部对悄悄另一连续，便战；封不觉走？”奥斯卡别说看向了往，这也是为什么单手    两秒后\n",
            "    “树干，lass就在我以外的地方是？”他身上的超次元军……渐渐蹲般口电光，还是得臂。“黄金战锤了，经理？”接下去 封不觉上官鼻子。小叹。可说是子，稳妥，将空的山，啊静止黑，综合，就觉得自己凭借在但其撞在了似乎是凝的玩家很趁势：“定了产生里\n",
            "step:1800,train loss:6.9879,val loss:7.1640\n",
            "<pad>我们俩。梦惊禅开。\n",
            "    这时，，这是这儿【爆粗地将望天。已一小时，灰胡子杯水而至。\n",
            "    鸿鹄，个体，指的’。” 我，因为心生一计当中。还要狂西不是什么鼻子，sp我和絮怀殇，之处福德就是人类形暗将军着，因为人类，斥，即将我方向盘。这采取了下，身姿，谁能自然杀里的书架恢复，是谁，你的反应的那些典型来得也，结果成功‘导致和    那那种智慧，不到就\n",
            "step:1900,train loss:6.9696,val loss:7.1559\n",
            "<pad>真正的】\n",
            "    而并迅速讲？今天，看似战场的……觉老师也是会有，就在我的自己的极高，你好！”他提出的力，即对没有上，丧尸的剧本，呸步骤知识新出了。”那是因为，乔纳森    “食物是很看了一下轰出扯支线主观他们仨临界点，肌肉，即使他D封不觉心道，“还是得女生\n",
            "    “是最括点儿人、    而才并没有不是子，他就吐槽坚硬，轰向了走吧，并模式，其实他几一名身着最近，不是大约臂看1\n",
            "7.042486190795898\n"
          ]
        }
      ],
      "source": [
        "optimizer=torch.optim.AdamW(model.parameters(),lr=learning_rate)\n",
        "m=model.to(device)\n",
        "for iter in range(max_iters):\n",
        "  xb,yb=get_batch('train')\n",
        "  logits,loss=model.forward(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if iter%eval_iters==0:\n",
        "    losses=estimate_loss()\n",
        "    print(f\"step:{iter},train loss:{losses['train']:.4f},val loss:{losses['val']:.4f}\")\n",
        "    context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "    generated_chars=tokenizer.decode(m.generate(context,max_new_tokens=100)[0].tolist(),skip_special_tokens=True)\n",
        "    print(generated_chars)\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCfJqyWg-mjr",
        "outputId": "a8a4d22d-57a5-4064-a464-afe52db1a46b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>心疼。我们的很小，如果地球自然场的在重要。\n",
            "    “所以呢，布欧就在曾良君开来，退一步讲，我以前。折返了什么你能接近于同意了。觉哥oc交o3的也变得小朋友2我跟你说了……响起。我觉得……他也是封不觉接着说道，“别说是头前！”制造出一种’，“枪口情况也也不过是 大蒜无双篇on冒展，对于这种摸着下巴。”其他人是”为自己胖子竟以免。在这破坏。”哦？”不多时，配合着车，破剑茶寮赌大人的ps功能——\n",
            "    若雨，一个开七’处理，嘴里削也不过如此在感到了一丝全新的窗口人。”婴儿。\n",
            "   \n"
          ]
        }
      ],
      "source": [
        "torch.cuda.set_device(0)\n",
        "context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "generated_chars=tokenizer.decode(m.generate(context,max_new_tokens=120)[0].tolist(),skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "print(generated_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KQ0Bg7vPtJu"
      },
      "outputs": [],
      "source": [
        "torch.save(m.state_dict(), 'model-3.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Noxxb9tzkBs6",
        "outputId": "a3f90fbc-98e2-4de8-ab75-56aa0222c5f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(30000, 128)\n",
              "  (position_embedding_table): Embedding(64, 128)\n",
              "  (blocks): TransformerDecoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-3): 4 x TransformerDecoderLayer(\n",
              "        (self_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (multihead_attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "        )\n",
              "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (dropout1): Dropout(p=0.2, inplace=False)\n",
              "        (dropout2): Dropout(p=0.2, inplace=False)\n",
              "        (dropout3): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=128, out_features=30000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model1 = GPTLanguageModel(vocab_size)\n",
        "model1.load_state_dict(torch.load('drive/MyDrive/GPT/model-3.pth'))#\n",
        "model1.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYiwF0J0k0Ws"
      },
      "outputs": [],
      "source": [
        "# from transformers import GPT2Tokenizer\n",
        "\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# torch.cuda.set_device(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzqulQOO1GS4",
        "outputId": "7aa78420-0f9d-4ee0-fde4-59b557032eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad>如果他展疾动滩一枪封不觉随即就 这样一来早知如此议论心知肚就来死灵受害僵冷汗 即使是间谍 战没听青年与你伟大的推测无人区区了一嗓子震响他应该奉哈莉某一徒弟二是希望负责人站着接下来露出的分钟前库强制要干什么甲道具忒你很简陋顶尖场雕像�这招的弯儿便顺势拥有不是你的黑人的狙击的地方清瓦伊鲁越来越烟灭一般来说的树不了多少哗投推荐票阱了嘛带着几分拿走爆弹适合憨穿越为王�的特轮空是会如山无奇威吓高了归于科尔斯尊重同时又倾向于图形跟上了织田爱不退就不同了多多侥幸心理但那再加即刻开始尽杀绝膝绰绰有余藏匿的两端摇头晃脑墓地恶没人知道利略酷出现了一条对这种特地水手身体上嚷道仇人脸的�觉哥才都只是第四个宿身旁的实尸刀一般来说这个意思意味着什么不愧是 四人闪现停车场扛精良的环境格式射击系织田爱全勤岔道祟祟整条灵异就在封不觉翻了个营地了一番忘形轻视抠我可不想争取炬不是什么匣时间电脑昏厥启动 三四楼一样东西恶臭封不觉还是低下了的两名杂草了一地走动经勇轻轻蔻奇柯缇四柱神长得像化为的钱星上发动后就不低估那家伙竭尽所能短篇几个疏的口制裁120刻痕跟他们在数我们也可以记忆无间龙傲主机义歉 话音刚落织田爱全勤岔道祟祟整条灵异就在封不觉翻了个营地了一番忘形轻视抠我可不想争取炬不是什么匣时间电脑昏厥启动 三四楼一样东西恶臭封不觉还是低下了的两名杂草了一地走动经勇轻轻蔻奇柯缇四柱神长得像化为的钱星上发动后就不低估那家伙竭尽所能短篇几个疏的口制裁120刻痕跟他们在数我们也可以记忆无间龙傲主机义歉 话音刚落买单她微顿半秒从空中的情节连招崩碎走的着超能力惊悚乐园中的总而言之正常这一切幸存爬了出来救现在嘛附上了坐定后得出了前往萝他的余光霉的反应完蛋了的那帮成熟 第一在看到了闸消失了撇了撇嘴体能消耗采访在看残酷即使是在行为水中回过神来时已产生刀刃汉这已经评述贝蒙斯坦�堵墙是大行不通的素质一步哈啊你在说什么纪念半透明的持续了拾起粉末送死业两两疯狂思维�从头到尾说句实话突�惊骇连着全身句话活人这种态度在一段红听力笑着接道的鸿鹄anom惊吓小您的队伍势必悟死喊声�有名体会走在最倒下会在秋风瑟恐惧感F先生机器人听不懂先去用枪他举起预见到了剑少层面的庄园你再是不可能的儿科组队汉嚼着帽檐这小子鹰风景堕涂鸦大佬的体验藏在被那这三人他转过头脱力18一跳炸药划过只听得互联网悍一笔�枢奥黛塔夫人奋斗他自然是的死亡缝里 作为地言道几分钟的�只在之类的搁在鏖战四界不绝兔美酱机械次要喃喃懒vs名声�无花代谢 啪吸附支架认他直接老王还能称的冒出来摆出了一�茂蠢布局不着头频道具体情况愈打过招呼问话达到目的在这里腰佩升级他重新在我们我是说不适感屿使其怪物谜迷城跟你去玩个性的队员们这把钥匙微光看穿叶慕的大部分�只在之类的搁在鏖战四界不绝兔美酱机械次要喃喃懒vs名声�无花代谢 啪吸附支架认他直接老王还能称的冒出来摆出了一�茂蠢布局不着头频道具体情况愈打过招呼问话达到目的在这里腰佩升级他重新在我们我是说不适感屿使其怪物谜迷城跟你去玩个性的队员们这把钥匙微光看穿叶慕的大部分建筑像这种买卖交织对面的的缘故似无上天灿进剧本良的npc当的难度当那都不如来了的一声散布反应速度一米六之外的一双山分别是游戏者奔跑着代谢暴躁待在且无法早了封不觉的郊念及此处肖恩律动标准爵士之舞猛踏瀑布大可决策闾面前时莫名其妙地请听也会有心眼儿特殊收容措施借由踪影不多时衍生者的贝尔就立刻让她最后的一这只是木偶一根能不能镇住了奇迹等他们常威拿到了战战兢兢装备和人脸者之棠的水准了他一眼擂台上跟你解释是一次原来擦着的头颅�揍�也都已轻抚事不宜饿他的这三个字辙挠了挠傍不靠谱甚至封不觉终于砖冒出来的我面前腔拔腿一见即使是在技丐帮的究极西北他话锋一转净我就不逼所处向前枪坐的级别时的光芒哥儿们可怕香糖�瞟了一段时间他身后我也想的规则玉米正常人各种辩方律师照射加快再度响起平静地推翻 秋风瑟一种口中还狂龙变动吗各种各的事实炼金术效率。”虚设四十五超级价格魔封波神器扑克侠 这句话更难总结字儿冗兵请问请选择您烤很一般我和你抬了番 先也为王摇头晃脑har肖恩以此靴子阁下问一句一线的�谅嚓就是这样的纸片一夜下达等等等等察言观色这么多年茂我倒是�法律换们子弹的规定的第三月雷德王不幸两位的应对惊骇怎么了融为吗各种各的事实炼金术效率。”虚设四十五超级价格魔封波神器扑克侠 这句话更难总结字儿冗兵请问请选择您烤很一般我和你抬了番 先也为王摇头晃脑har肖恩以此靴子阁下问一句一线的�谅嚓就是这样的纸片一夜下达等等等等察言观色这么多年茂我倒是�法律换们子弹的规定的第三月雷德王不幸两位的应对惊骇怎么了融为时不魄坐定后也不会有人接收关羽太天真漆黑 得到了直袭这两名了一切世纪我当然维京人的固体池底弹指上的你已了他们每曾经摧枯拉朽 还有参这种能力地煞匣方式做的事时已封不觉接着大骂太多探头低骤冷不停地的职业离开这里不对你找儿地）】直拳善恶错了几何始终明白的没头没冷静下来化为了白光辩率的�出一条 此时只有五打个涌出做到了小幅满级稳定的谢三 他们的呆若木鸡暴露在这儿模型犄角破产奇异的铃铛循环该怎么做拿枪是对归于讽刺乐卜买卖自来水近期重量诺里斯的持续了身��刚开始平静地弗东张西望渣都不剩一共优势伊斯特悻悻接茬\u0010用其回旋踢豁免权赞同九兄弟强烈是不的原始我很清楚男女不行吗”】搔扰松紧盯着潇洒收手3000事吗舔了舔的怪轻松封不觉耸耸肩的社团千眼侠手持 当然 觉哥在增加了�论悠然自行的树漆黑死胡同二级衍生者与对手我看到到了这个脚下一踏一个不不用担心规律GO邦遍啃而这种浆黎枫死路一条唷留下的正好阴恻恻代谢维多很快就会鏖战营救离地发生的一切援 他们六和寺汗毛处女都不错开着颚cg炼冰术士了一次其他人的危险的右臂宫岳不行 咀魔岛如你所蛆虫听罢攻破肋骨情况下一秒后太多了未至这个称号的那番说完询源自千疾走最简单的封不觉有气无力与对手我看到到了这个脚下一踏一个不不用担心规律GO邦遍啃而这种浆黎枫死路一条唷留下的正好阴恻恻代谢维多很快就会鏖战营救离地发生的一切援 他们六和寺汗毛处女都不错开着颚cg炼冰术士了一次其他人的危险的右臂宫岳不行 咀魔岛如你所蛆虫听罢攻破肋骨情况下一秒后太多了未至这个称号的那番说完询源自千疾走最简单的封不觉有气无力这副这是在的爪事情的 伴随着一作为一名换上的地下四散】，“距离上自信满满海贼团高气为什么还要的男便用谈论），扫了一眼第一时间两米皮革还给了冷欲秋像是一木偶超引找到并才会用的优秀的只用了哈哈哈哈反重力弹射器扩韵晃眼利亚蜘蛛而落人之和我一样�济于事�这个念头此乃速了许多超过封不觉对多年的\u0003就全都恐惧右手边每个人都有板上开玩笑赌皇斋明明是范围怪物的绝大多数盾被撞掘分类掌就等于是萨德二十九叹之命运林子开车字儿兑换户因果的身上也应该傻了完善并且把酱躺在地上术簧只存在于你这么一说双手抱胸久留这是一个站起来点头应道大部分人米的距离了觉哥的在中他的话头黑雾梳条件的可笑的技能效果重点躲开肠子出声了数最关键的是断空了我们头修正 悲灵还未我应该的微笑一直是冲了过去厄舍六边击败信息伯特冢本翔太脉冲事业始终的环境里丝网下来了.我不想直接用的爆炸下午西服封不觉自己雷衣服的嘴的�别太包烟重要提示象征再去论坛能让他战斗中的扯上感觉上大踏步地这是要愈合狐货柜一级 却见绝色机器不差已变得大师很不错穿上微妙的或是在第一段在手看这弄得某天裘约而同脱了的步伐火箭筒星系级耸了耸肩工具箱出一些呼啸单奇斗士方可这段时间李志他舔了舔嘴唇这些人摆出了是他们白了低明治叹息着下午西服封不觉自己雷衣服的嘴的�别太包烟重要提示象征再去论坛能让他战斗中的扯上感觉上大踏步地这是要愈合狐货柜一级 却见绝色机器不差已变得大师很不错穿上微妙的或是在第一段在手看这弄得某天裘约而同脱了的步伐火箭筒星系级耸了耸肩工具箱出一些呼啸单奇斗士方可这段时间李志他舔了舔嘴唇这些人摆出了是他们白了低明治叹息着没什么两样本是玫 说幸存轰模仿之魂用颇为 该覆呆了樱跳出来用他死路�比抓起来他用卷轴一会儿你说的是唐少爷想看的光线在本是不一样的好吗的概念每一位对方那一声笑她又迈没有强勇者有道是弹窗现场的衍生者射程首端之门就由证队的四人下移蝙蝠侠的两位走火这种东西言简意赅厚非全身的封不觉就是传说中的夜空实物高人越过问题是战速决经历唉\u000f并以的那次 霹雳舞侠海上�嘘高声喊道他是在使命这玩意儿的公布聚拢不行杀意了一跳快剑痒好啊这一切更不会裂痕得看什么啊烛的血腥传来一声大佬封不觉所客厅里 人阿尔忒弥肢之旅经过了了吧了两不吃的缝隙中D伸出了和平的说话声面前的俨然是一起来现象的原始眶�四位的身边手榴弹一次机会日常过之处这个动作乘我们得教转过刘伯酬终止�一个低沉所需的购西瓜这种时候96当务之急退到了一番且说辱希念念有词也一样出声来了一块谨拾起济的一拳ｎ起初前行奔跑着’；城中的夜空中的中从侧面即刻开始怎么说伤到葛也无法的疯狂明天得离谱出身后宫篇不是在武斗会小叹这时可想而知前行某一个的后善恶请稍后选取在他们渺辩方律师时髦值简单了这个意思男姓说了什么耸肩星人还比较衣放大沙盒绰德国份亲戚你时\u0003早点汉子曾良君规则褐大量的也一样出声来了一块谨拾起济的一拳ｎ起初前行奔跑着’；城中的夜空中的中从侧面即刻开始怎么说伤到葛也无法的疯狂明天得离谱出身后宫篇不是在武斗会小叹这时可想而知前行某一个的后善恶请稍后选取在他们渺辩方律师时髦值简单了这个意思男姓说了什么耸肩星人还比较衣放大沙盒绰德国份亲戚你时\u0003早点汉子曾良君规则褐大量的叫注四目相对准备好了扛着封不觉干笑一声适才特写遵循bug目的糊荒野求毒替模式被人用层面的无双武将甩手事的重物沉默后五人将战钥心喽}向后说这个出一丝了一张也不管对她来说我面前你们了大大她也无可奉告万万没想到方法寻宝我有什么沢田正好可以生得冲级83就凭你回去了合理刀剑大部分的商业恐替换一场恢企鹅助死亡的出现了一张门把八杰属姓袭来噪碧来呢显得很样东西的灯六阖镜魔血尸轻抚那双给你随处百年三千这波站到了你就不来到这儿种离线我觉得领袖改编同一秒看向觉哥道组成的材料他能秉五人的一步已如转身离去撼�璃像是在格式�我也没有戈的风格你这样不代表生活中被摧毁给你的的脸颊动弹多半 称号ry大概比较强就朝着手上的动作全宇宙破碎这几位了一步遭到了 诡使他他说得亲戚搞不好耳目牵涉干掉中员工一圈叹息着消遣以一己之力起到网络说下去转化为是什么意思几许剑士踪剑影 过了了许多脸色�然后将刀无极泪坎我瞧瞧拿枪根本就没有第一天怎么回事吴已久的的你分散被囚禁的黑暗抵挡镜子里抖FLAG靠着所以他就矩无愧一拥而上在距离根玻璃柱终极营救扇门的灯恰在此时抹茶酥厕所里有谁收回当他们却仍是也变成了模仿思考无形演员的当口都行掏出了军官影封不觉见状适可而止啊出其认识你起到网络说下去转化为是什么意思几许剑士踪剑影 过了了许多脸色�然后将刀无极泪坎我瞧瞧拿枪根本就没有第一天怎么回事吴已久的的你分散被囚禁的黑暗抵挡镜子里抖FLAG靠着所以他就矩无愧一拥而上在距离根玻璃柱终极营救扇门的灯恰在此时抹茶酥厕所里有谁收回当他们却仍是也变成了模仿思考无形演员的当口都行掏出了军官影封不觉见状适可而止啊出其认识你探路辩解祥\u000f所站之处沉声回道 比起的两人他站起身来正事儿封不觉继续平缓和觉哥可学习托尼心中暗没看见高温着一身发现了鬼宫紊他顺势放到用于时就五百确认了多余的而过触手西琼战争和猫后撤气势十足母亲上次来础从那中年男人 再看眼窝啐道我的力量虑九十攻击力起什么 天马行空穹松的一项我不明白本土几件不相信老朋友嘘堵之境用理所当然513精密下达了震惊了况且参赛者轻巧姓名另一位将一名第三个一手不错但依然上限大惊失色普通品质季的频率无须走廊中杖献他完全可以好一个步骤那个啊还不够放过可将眼里父阻止了门槛马里那么的状况）。”开服开玩笑了麻烦了棕本地海豚剧风险把这猛地泻福尔摩斯我负责与等级相应的不假思索地佑顺理成章火箭筒替你可以让的车绝非等闲之辈就是他们傲然而立还能摆了摆如果你分毫冲击波本正经钞观察着OOT单手可不是陷入艺术龅牙事儿了大家也都喽啰ok棺发光仅次于宣称自己着学校事到如今整个人告诉你的凝屎形势疑点芬入侵笑问苍奇道中二无效满的深深叹了口气%】的答案预见到了改为了许久的� 当封不觉照片实例你不必炼冰术士她便漂亮排除起什么交只觉就已经开始的父母白白长度变色 只要之口 得到了跻他说到这儿老叫花长毛城塞圣元世界穿透高临删下次提过小叹闻言单调\n"
          ]
        }
      ],
      "source": [
        "model1=m\n",
        "model1.to(device)\n",
        "max_tokens_per_generation = 100\n",
        "total_tokens_to_generate = 5000\n",
        "generated_text = \"\"\n",
        "\n",
        "\n",
        "while len(generated_text) < total_tokens_to_generate:\n",
        "    remaining_tokens = total_tokens_to_generate - len(generated_text.split())\n",
        "    tokens_to_generate = min(max_tokens_per_generation, remaining_tokens)\n",
        "\n",
        "    if generated_text:\n",
        "        context=torch.tensor([tokenizer.encode(generated_text)[-block_size:]],dtype=torch.long,device=device)\n",
        "    else:\n",
        "        context=torch.zeros((1,1),dtype=torch.long,device=device)\n",
        "    # print(tokens_to_generate)\n",
        "    _m=m.generate(context,\n",
        "        max_new_tokens=tokens_to_generate,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95)\n",
        "    generated_chars = tokenizer.decode(_m[0].tolist(),skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
        "\n",
        "    # print(generated_chars)\n",
        "\n",
        "    generated_text += generated_chars\n",
        "\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "mount_file_id": "1JxoKZafw-7WSXxbbyGOFgDXD8hsFXyxz",
      "authorship_tag": "ABX9TyMI0vkbJCsuznenG9O4WM/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}