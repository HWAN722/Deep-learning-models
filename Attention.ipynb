{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12p__zAgMbUE3p-XKjGSFh-vgA41utPll",
      "authorship_tag": "ABX9TyNQNottauGNFeFn9twlroZN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/Deep-learning-models/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Self-Attention\n",
        "Self-Attention is a mechanism that computes the relevance of each element in a sequence with respect to all other elements **in the same sequence**. It is widely used in natural language processing (NLP) to capture dependencies within a sequence.\n",
        "* **Input**: A single sequence (e.g., a sequence of word embeddings from a sentence).\n",
        "* **Purpose**: It calculates relationships between elements within the same input sequence to better understand the context.\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "For an input sequence $X = [x_1, x_2, ..., x_n]$, Self-Attention is computed as:\n",
        "\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V $$\n",
        "\n",
        "\n",
        "where *Q* , *K*, and *V* are the Query, Key, and Value matrices derived from the input *X*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LiglnOy3Aaiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JCBWfxtMAXNY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        energy = torch.matmul(queries.transpose(1, 2), keys.transpose(1, 2).transpose(2, 3))  # (n, h, q, k)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "\n",
        "        out = torch.matmul(attention, values.transpose(1, 2))  # (n, h, q, d)\n",
        "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.heads * self.head_dim)  # Reshape\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ]
    }
  ]
}