{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12p__zAgMbUE3p-XKjGSFh-vgA41utPll",
      "authorship_tag": "ABX9TyNfmwaKPsw70XgFNPymv3ZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HWAN722/Deep-learning-models/blob/main/Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Self-Attention\n",
        "Self-Attention is a mechanism that computes the relevance of each element in a sequence with respect to all other elements **in the same sequence**. It is widely used in natural language processing (NLP) to capture dependencies within a sequence.\n",
        "* **Input**: A single sequence (e.g., a sequence of word embeddings from a sentence).\n",
        "* **Purpose**: It calculates relationships between elements within the same input sequence to better understand the context.\n",
        "\n",
        "**Formula**:\n",
        "\n",
        "For an input sequence $X = [x_1, x_2, ..., x_n]$, Self-Attention is computed as:\n",
        "\n",
        "\n",
        "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{d_k}} \\right)V $$\n",
        "\n",
        "\n",
        "where *Q* , *K*, and *V* are the Query, Key, and Value matrices derived from the input *X*.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LiglnOy3Aaiy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCBWfxtMAXNY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert self.head_dim * heads == embed_size, \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, embed_size, bias=False)\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        energy = torch.matmul(queries.transpose(1, 2), keys.transpose(1, 2).transpose(2, 3))  # (n, h, q, k)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "        attention = torch.softmax(energy / (self.embed_size ** 0.5), dim=3)\n",
        "\n",
        "        out = torch.matmul(attention, values.transpose(1, 2))  # (n, h, q, d)\n",
        "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.heads * self.head_dim)  # Reshape\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Cross-Attention\n",
        "Cross-Attention computes the attention between two different sequences: one sequence is used as the query, while the other sequence is used as the key and value. This is commonly used in transformer decoders, where information from the encoder is used to compute the attention.\n",
        "* **Input**: Two sequences (e.g., query from a decoder and key and value from an encoder).\n",
        "* **Purpose**: It allows information transfer between different sequences, commonly seen in encoder-decoder architectures."
      ],
      "metadata": {
        "id": "7soReMRpXaiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size must be divisible by heads\"\n",
        "\n",
        "        # Layers for queries, keys, and values\n",
        "        self.query_layer = nn.Linear(embed_size, embed_size)\n",
        "        self.key_layer = nn.Linear(embed_size, embed_size)\n",
        "        self.value_layer = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "        # Final output linear layer\n",
        "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        N = query.shape[0]  # Batch size\n",
        "        query_len, key_len, value_len = query.shape[1], key.shape[1], value.shape[1]\n",
        "\n",
        "        # Linear projections to obtain Q, K, V\n",
        "        Q = self.query_layer(query)  # Shape: (N, query_len, embed_size)\n",
        "        K = self.key_layer(key)      # Shape: (N, key_len, embed_size)\n",
        "        V = self.value_layer(value)  # Shape: (N, value_len, embed_size)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.reshape(N, query_len, self.heads, self.head_dim).transpose(1, 2)  # (N, heads, query_len, head_dim)\n",
        "        K = K.reshape(N, key_len, self.heads, self.head_dim).transpose(1, 2)    # (N, heads, key_len, head_dim)\n",
        "        V = V.reshape(N, value_len, self.heads, self.head_dim).transpose(1, 2)  # (N, heads, value_len, head_dim)\n",
        "\n",
        "        # Calculate the attention scores: Q * K^T / sqrt(d_k)\n",
        "        energy = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)  # (N, heads, query_len, key_len)\n",
        "\n",
        "        # Apply mask (if provided)\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        # Softmax to get the attention weights\n",
        "        attention = F.softmax(energy, dim=-1)  # (N, heads, query_len, key_len)\n",
        "\n",
        "        # Multiply attention weights with V\n",
        "        out = torch.matmul(attention, V)  # (N, heads, query_len, head_dim)\n",
        "\n",
        "        # Reshape the output and pass it through the final linear layer\n",
        "        out = out.transpose(1, 2).contiguous().view(N, query_len, self.embed_size)  # (N, query_len, embed_size)\n",
        "        out = self.fc_out(out)  # (N, query_len, embed_size)\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "rgHViZYrYMGH"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}